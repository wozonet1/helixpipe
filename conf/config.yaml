
defaults:
  - data: DrugBank # Default dataset to use
  - predictor: rgcn
  - relations: DP_sim
  - params: default
  - training: rgcn-random
  - analysis: split_diff # [核心新增] 添加这一行
  - _self_  # This line is important, it tells Hydra to also load the rest of this file.



data:
  subfolders:
    raw: "raw"
    processed: "processed"
  use_gtopdb: false
  files:
    raw:
      dti_interactions: "full.csv"
    processed:
        typed_edge_list_template: "typed_edges-{relations_suffix}.csv"
        abbr: 
          dp_interaction: dp
          lp_interaction: lp
          pp_similarity: pp
          dd_similarity: dd
          dl_similarity: dl
          ll_similarity: ll
        link_prediction_labels_template: "dl_p_edges{split_suffix}.csv"
        nodes_metadata: "nodes.csv"
        node_features:  "node_features.npy"
        feature_caches:
          molecule_embeddings: "feature_caches/molecule_features.pkl"
          protein_embeddings: "feature_caches/esm_protein_features.pkl"
        similarity_matrices:
          molecule: "sim_matrixes/dl_similarity_matrix.pkl"
          protein: "sim_matrixes/prot_similarity_matrix.pkl"
        indexes:
          drug: "indexes/drug2index.pkl"
          ligand: "indexes/ligand2index.pkl"
          protein: "indexes/prot2index.pkl"
        suffix:
          train: "_train"
          test: "_test"
    gtopdb:
      raw:
        interactions: "interactions.csv"
        ligands: "ligands.csv"
        targets: "targets_and_families.csv"
      processed:
        ligands: "gtopdb_ligands.csv"
        proteins: "gtopdb_proteins.csv"
        interactions: "gtopdb_p-l_edges.csv"
  feature_extractors:
    protein:
      extractor_function: "extract_esm_protein_embeddings"
      model_name: "facebook/esm2_t30_150M_UR50D" 
      # 在GPU上进行特征提取时的批处理大小
      batch_size: 32
    molecule:
      extractor_function: "extract_chemberta_molecule_embeddings"
      model_name: "seyonec/ChemBERTa-zinc-base-v1"
      batch_size: 64 # 通常可以比蛋白质的batch size更大

# --- 2. Global Parameters (that don't belong to a specific component) ---
# These are mostly parameters for data processing and evaluation
training:
    learning_rate: 0.005
    epochs: 150
    batch_size: 2048
    negative_sampling_ratio: 1
    weight_decay: 1e-5

runtime:
  seed: 514
  cpus: 100
  gpu: "cuda:2"
  force_restart: false
  sim_restart: false
  debug: true
  trian_loader_cpus: 16
  test_loader_cpus: 8
  validate_every_n_epochs: 10

mlflow:
  tracking_uri: "logs/mlruns" 
  experiment_name: "Het-DTI_Prediction_Hydra_on_${data.primary_dataset}"

hydra:
  run:
    dir: logs/outputs/${now:%Y-%m-%d}/${now:%H-%M-%S} #相当于前面自动有${hydra.runtime.cwd}

  # 您也可以在这里配置 multirun 的输出目录
  sweep:
    dir: logs/multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${hydra.job.num} # 每个子任务的文件夹名