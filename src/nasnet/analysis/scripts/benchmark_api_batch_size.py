# æ–‡ä»¶: src/nasnet/analysis/scripts/benchmark_api_batch_sizes.py (å…¨æ–°)

import argparse
import gzip
import random
import time

import numpy as np
import pandas as pd
import research_template as rt
from tqdm import tqdm

# --- åŠ¨æ€è·¯å¾„è®¾ç½® ---
try:
    PROJECT_ROOT = rt.get_project_root()
except IndexError:
    raise RuntimeError("Could not determine project root.")

# --- å¯¼å…¥æˆ‘ä»¬çš„fetcherå‡½æ•° ---
from nasnet.data_processing.services.canonicalizer import (
    fetch_sequences_from_uniprot,
    fetch_smiles_from_pubchem,
)

# --- ä»£ç†é…ç½® ---
# å¦‚æœéœ€è¦ï¼Œåœ¨è¿™é‡Œè®¾ç½®æ‚¨çš„ä»£ç†
PROXY_CONFIG = None
# PROXY_CONFIG = {'http': 'http://127.0.0.1:7890', 'https': 'http://127.0.0.1:7890'}

# ==============================================================================
# 1. é’ˆå¯¹å…·ä½“APIçš„Benchmarkâ€œå·¥äººâ€å‡½æ•°
# ==============================================================================


def benchmark_uniprot(batch_size: int, num_trials: int, id_pool: list) -> dict:
    """å¯¹ UniProt API è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚"""
    print(f"\n--- Testing UniProt | Batch Size: {batch_size} ({num_trials} trials) ---")
    timings = []
    success_counts = 0

    for i in range(num_trials):
        id_batch = random.sample(id_pool, k=min(batch_size, len(id_pool)))
        start_time = time.time()

        results = fetch_sequences_from_uniprot(id_batch)

        duration = time.time() - start_time
        timings.append(duration)
        success_counts += len(results)

        print(
            f"  - Trial {i + 1}/{num_trials}: Fetched {len(results)}/{len(id_batch)} sequences in {duration:.2f}s"
        )
        time.sleep(1)  # æ¯æ¬¡è¯•éªŒé—´ä¼‘æ¯

    avg_time = np.mean(timings) if timings else float("inf")
    avg_success_rate = (
        (success_counts / (num_trials * batch_size)) * 100
        if num_trials * batch_size > 0
        else 0
    )

    return {
        "batch_size": batch_size,
        "avg_time_s": avg_time,
        "success_rate_%": avg_success_rate,
    }


def benchmark_pubchem(batch_size: int, num_trials: int, id_pool: list) -> dict:
    """å¯¹ PubChem API è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚"""
    print(f"\n--- Testing PubChem | Batch Size: {batch_size} ({num_trials} trials) ---")
    timings = []
    success_counts = 0

    for i in range(num_trials):
        id_batch = random.sample(id_pool, k=min(batch_size, len(id_pool)))
        start_time = time.time()

        results = fetch_smiles_from_pubchem(
            id_batch, batch_size=batch_size, proxies=PROXY_CONFIG
        )

        duration = time.time() - start_time
        timings.append(duration)
        success_counts += len(results)

        print(
            f"  - Trial {i + 1}/{num_trials}: Fetched {len(results)}/{len(id_batch)} SMILES in {duration:.2f}s"
        )
        time.sleep(1)

    avg_time = np.mean(timings) if timings else float("inf")
    avg_success_rate = (
        (success_counts / (num_trials * batch_size)) * 100
        if num_trials * batch_size > 0
        else 0
    )

    return {
        "batch_size": batch_size,
        "avg_time_s": avg_time,
        "success_rate_%": avg_success_rate,
    }


# ==============================================================================
# 2. IDæ± ç”Ÿæˆå™¨
# ==============================================================================


def get_uniprot_id_pool(size=2000) -> list:
    """ç”Ÿæˆä¸€ç»„éšæœºä½†æ ¼å¼æœ‰æ•ˆçš„UniProt IDã€‚"""
    print(f"\n--> Generating a pool of {size} random UniProt IDs...")
    prefixes = ["P", "Q", "O"]
    # ç”Ÿæˆæ›´å¤šæ ·çš„IDæ ¼å¼
    pool = {
        f"{random.choice(prefixes)}{random.randint(10000, 99999)}"
        for _ in range(size // 2)
    }
    pool.update(
        {
            f"A0A{random.randint(100, 999)}R{random.randint(1, 9)}R{random.randint(1, 9)}"
            for _ in range(size // 2)
        }
    )
    return list(pool)


def get_pubchem_cid_pool(size=2000) -> list:
    """ç”Ÿæˆä¸€ç»„éšæœºçš„PubChem CIDã€‚"""
    print(f"\n--> Generating a pool of {size} random PubChem CIDs...")
    # åœ¨ä¸€ä¸ªå¸¸è§çš„èŒƒå›´å†…ç”Ÿæˆéšæœºæ•´æ•°
    return [random.randint(1, 100000) for _ in range(size)]


def load_uniprot_id_pool_from_assets() -> list:
    """ä» data/assets/uniprotkb_proteome...tsv æ–‡ä»¶ä¸­åŠ è½½çœŸå®çš„UniProt IDã€‚"""
    filepath = PROJECT_ROOT / "data" / "assets" / "uniprotkb_proteome_UP000005640.tsv"
    if not filepath.exists():
        raise FileNotFoundError(f"UniProt proteome file not found at: {filepath}")

    print(f"\n--> Loading REAL UniProt ID pool from: {filepath.name}...")
    df = pd.read_csv(filepath, sep="\t", usecols=["Entry", "Reviewed", "Organism (ID)"])
    df_human_reviewed = df[
        (df["Organism (ID)"] == 9606) & (df["Reviewed"] == "reviewed")
    ]
    ids = df_human_reviewed["Entry"].unique().tolist()

    print(f"--> Loaded {len(ids)} unique, reviewed, human UniProt IDs.")
    return ids


def load_pubchem_cid_pool_from_assets(sample_size: int = 50000) -> list:
    """ä» data/assets/CID-Synonym-filtered.gz ä¸­éšæœºæŠ½æ ·çœŸå®çš„PubChem CIDã€‚"""
    filepath = PROJECT_ROOT / "data" / "assets" / "CID-Synonym-filtered.gz"
    if not filepath.exists():
        raise FileNotFoundError(f"PubChem synonym file not found at: {filepath}")

    print(f"\n--> Loading REAL PubChem CID pool by sampling from: {filepath.name}...")

    # ç”±äºæ–‡ä»¶å·¨å¤§ï¼Œæˆ‘ä»¬ä¸è¯»å–å…¨éƒ¨ï¼Œè€Œæ˜¯è¿›è¡ŒéšæœºæŠ½æ ·
    cids = set()
    with gzip.open(filepath, "rt", encoding="utf-8") as f:
        # ä¼°ç®—æ€»è¡Œæ•°ä»¥è¿›è¡Œåˆç†çš„éšæœºæŠ½æ ·
        estimated_total = 300_000_000
        # æˆ‘ä»¬å¸Œæœ›é‡‡æ ·å¤§çº¦0.1%çš„è¡Œæ¥è·å¾—è¶³å¤Ÿå¤šçš„ID
        sampling_rate = sample_size / estimated_total

        for line in tqdm(f, total=estimated_total, desc="   - Sampling CIDs"):
            if random.random() < sampling_rate:
                try:
                    cid_str, _ = line.strip().split("\t", 1)
                    cids.add(int(cid_str))
                except (ValueError, IndexError):
                    continue

    ids = list(cids)
    print(f"--> Sampled {len(ids)} unique PubChem CIDs.")
    return ids


# ==============================================================================
# 3. ä¸»åè°ƒå‡½æ•°
# ==============================================================================


def main():
    parser = argparse.ArgumentParser(
        description="Benchmark API batch sizes using real IDs from data/assets."
    )
    parser.add_argument(
        "target", choices=["uniprot", "pubchem"], help="The API to benchmark."
    )
    args = parser.parse_args()

    print("=" * 80)
    print(" " * 20 + f"STARTING BATCH SIZE BENCHMARK FOR: {args.target.upper()}")
    print("=" * 80)

    BATCH_SIZES_TO_TEST = [50, 100, 200, 400, 500]
    NUM_TRIALS_PER_SIZE = 3

    # --- è‡ªåŠ¨åŠ è½½IDæ±  ---
    if args.target == "uniprot":
        id_pool = load_uniprot_id_pool_from_assets()
        benchmark_func = benchmark_uniprot
        success_col_name = "api_response_rate_%"
    else:  # pubchem
        id_pool = load_pubchem_cid_pool_from_assets()
        benchmark_func = benchmark_pubchem
        success_col_name = "success_rate_%"

    all_results = []
    for size in BATCH_SIZES_TO_TEST:
        result = benchmark_func(size, NUM_TRIALS_PER_SIZE, id_pool)
        all_results.append(result)

    print("\n\n" + "=" * 80)
    print(" " * 30 + "Benchmark Summary")
    print("=" * 80)

    results_df = pd.DataFrame(all_results)
    results_df["throughput_id_per_sec"] = (
        results_df["batch_size"] / results_df["avg_time_s"]
    )

    # è°ƒæ•´åˆ—åä»¥åæ˜ æ–°çš„æˆåŠŸç‡å®šä¹‰
    if args.target == "uniprot":
        results_df.rename(
            columns={"api_response_rate_%": "response_rate_%"}, inplace=True
        )

    print(results_df.to_string(index=False, float_format="%.2f"))

    # --- æ¨èæœ€ä½³é€‰æ‹© ---
    reliable_options = results_df[results_df[success_col_name] > 98]
    if not reliable_options.empty:
        best_choice = reliable_options.loc[
            reliable_options["throughput_id_per_sec"].idxmax()
        ]
        print("\n" + "=" * 80)
        print(f"ğŸ† Recommended Batch Size: {int(best_choice['batch_size'])}")
        print(
            "   This size offers the best throughput while maintaining a high response rate (>98%)."
        )
    else:
        print("\n" + "=" * 80)
        print(
            "âš ï¸ No batch size achieved a high response rate. Check for API issues or network throttling."
        )


if __name__ == "__main__":
    main()
