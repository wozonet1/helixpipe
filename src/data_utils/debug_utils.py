from torch_geometric.data import HeteroData
import torch
import research_template as rt
import pandas as pd
from omegaconf import DictConfig
from rdkit import Chem
import re
from tqdm import tqdm
from typing import Optional
import time
import requests
from collections import namedtuple, Counter
from joblib import Parallel, delayed
import hydra

# run.py æˆ– utils/config_validator.py
from pathlib import Path


def is_config_valid(cfg: DictConfig) -> bool:
    """
    ã€V2 é‡æ„ç‰ˆã€‘å¯¹æœ€ç»ˆç»„åˆå¥½çš„é…ç½®è¿›è¡Œä¸€ç³»åˆ—é€»è¾‘ä¸€è‡´æ€§æ£€æŸ¥ã€‚
    """
    print("\n--- [Config Validation] Running logic consistency checks... ---")
    checks_passed = True

    # --- æ£€æŸ¥ 1: è¾…åŠ©æ•°æ®é›†ä¸å›¾å…³ç³»çš„ä¾èµ–å…³ç³» ---
    # è§„åˆ™ï¼šå¦‚æœ data_params.auxiliary_datasets åŒ…å« 'gtopdb'ï¼Œ
    #       é‚£ä¹ˆ relations.flags ä¸­è‡³å°‘è¦æœ‰ä¸€ä¸ªä¸ 'ligand' ç›¸å…³çš„å¼€å…³æ˜¯ trueã€‚
    aux_datasets = cfg.data_params.get("auxiliary_datasets", [])
    if "gtopdb" in aux_datasets:
        relation_flags = cfg.relations.flags
        # å®šä¹‰å“ªäº›å…³ç³»ä¾èµ–äºGtoPdbï¼ˆå³å¼•å…¥äº†'ligand'èŠ‚ç‚¹ç±»å‹ï¼‰
        ligand_related_relations = ["lp_interaction", "ll_similarity", "dl_similarity"]

        is_any_ligand_relation_enabled = any(
            relation_flags.get(rel, False) for rel in ligand_related_relations
        )

        if not is_any_ligand_relation_enabled:
            print(
                "âŒ VALIDATION FAILED: GtoPdb is enabled as an auxiliary dataset, "
                "but no ligand-related relations (lp_interaction, ll_similarity, dl_similarity) are enabled in the `relations` config."
            )
            checks_passed = False

    # åå‘æ£€æŸ¥ï¼šå¦‚æœå¯ç”¨äº†ligandç›¸å…³å…³ç³»ï¼Œä½†gtopdbä¸åœ¨è¾…åŠ©æ•°æ®é›†ä¸­
    # ï¼ˆè¿™ä¸ªæ£€æŸ¥å¯èƒ½è¿‡äºä¸¥æ ¼ï¼Œæœ‰æ—¶æˆ‘ä»¬å¯èƒ½å¸Œæœ›åœ¨æ²¡æœ‰gtopdbçš„æƒ…å†µä¸‹ä¹Ÿå…è®¸dl_similarityï¼Œ
    #  ä½†ä½œä¸ºä¸€ä¸ªä¾‹å­ï¼Œå®ƒå¯ä»¥è¿™æ ·åšï¼‰
    relation_flags = cfg.relations.flags
    ligand_related_relations = ["lp_interaction", "ll_similarity", "dl_similarity"]
    is_any_ligand_relation_enabled = any(
        relation_flags.get(rel, False) for rel in ligand_related_relations
    )

    if is_any_ligand_relation_enabled and "gtopdb" not in cfg.data_params.get(
        "auxiliary_datasets", []
    ):
        print(
            "âš ï¸ VALIDATION WARNING: Ligand-related relations are enabled, but 'gtopdb' is not in `auxiliary_datasets`."
        )
        # è¿™é‡Œæˆ‘ä»¬å¯ä»¥åªæ‰“å°è­¦å‘Šè€Œä¸æ˜¯ç›´æ¥å¤±è´¥ï¼Œå› ä¸ºå¯èƒ½å­˜åœ¨åˆç†çš„ä½¿ç”¨åœºæ™¯
        # checks_passed = False

    # --- æ£€æŸ¥ 2: å¿…éœ€çš„åŸå§‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨ ---
    # è¿™ä¸ªæ£€æŸ¥éå¸¸é‡è¦ï¼Œå¯ä»¥é¿å…åœ¨æµæ°´çº¿ä¸­é€”å› æ–‡ä»¶ç¼ºå¤±è€Œå¤±è´¥

    # a. æ£€æŸ¥ä¸»æ•°æ®é›†çš„æƒå¨DTIæ–‡ä»¶
    primary_dti_path = rt.get_path(cfg, "data_structure.paths.raw.authoritative_dti")
    if not primary_dti_path.exists():
        print(
            f"âŒ VALIDATION FAILED: Primary authoritative DTI file for dataset '{cfg.data_structure.name}' not found."
        )
        print(f"   - Expected at: {primary_dti_path}")
        checks_passed = False

    # b. å¦‚æœå¯ç”¨äº†è¾…åŠ©æ•°æ®é›†ï¼Œä¹Ÿæ£€æŸ¥å®ƒä»¬çš„æ–‡ä»¶
    for aux_name in cfg.data_params.get("auxiliary_datasets", []):
        try:
            # åŒæ ·åˆ©ç”¨ä¸´æ—¶é…ç½®çš„æŠ€å·§æ¥è·å–è·¯å¾„
            with hydra.initialize_config_dir(
                config_dir=Path(cfg.hydra.runtime.config_dir).parent
            ):
                aux_cfg = hydra.compose(
                    config_name="config", overrides=[f"data_structure={aux_name}"]
                )
            aux_path = rt.get_path(
                aux_cfg, "data_structure.paths.raw.authoritative_dti"
            )
            if not aux_path.exists():
                print(
                    f"âŒ VALIDATION FAILED: Auxiliary authoritative DTI file for dataset '{aux_name}' not found."
                )
                print(f"   - Expected at: {aux_path}")
                checks_passed = False
        except Exception as e:
            print(
                f"âŒ VALIDATION FAILED: Could not resolve path for auxiliary dataset '{aux_name}'. Error: {e}"
            )
            checks_passed = False

    # --- æ€»ç»“ ---
    if checks_passed:
        print("âœ… All configuration logic checks passed.")
    else:
        print("\n--- [Config Validation] Finished with errors. Halting execution. ---")

    return checks_passed


def run_optional_diagnostics(hetero_graph: HeteroData):
    """
    Runs a suite of OPTIONAL but recommended diagnostic checks.
    Call this during development to ensure data integrity.
    """
    print("\n--- [OPTIONAL DIAGNOSTIC] Running full diagnostic suite ---")

    # Check 1: Official PyG validation
    hetero_graph.validate(raise_on_error=True)
    print("âœ… (1/3) Official PyG validation successful.")

    # Check 2 & 3: Deep health checks
    if not (
        diagnose_hetero_data(hetero_graph) and diagnose_node_features(hetero_graph)
    ):
        raise ValueError("HeteroData object failed deep health checks.")
    print("âœ… (2/3 & 3/3) Deep health checks successful.")

    print("--- [OPTIONAL DIAGNOSTIC] All checks passed. ---\n")


# æ”¾ç½®åœ¨ train.py çš„é¡¶éƒ¨
def diagnose_hetero_data(data: HeteroData):
    """ä¸€ä¸ªè¯¦ç»†çš„è¯Šæ–­å‡½æ•°ï¼Œå½»æŸ¥HeteroDataå¯¹è±¡çš„å¥åº·çŠ¶å†µã€‚"""
    print("\n--- [DIAGNOSTIC 2] Performing deep health check on HeteroData object...")
    is_healthy = True

    # æ£€æŸ¥1: æ‰€æœ‰edge_indexéƒ½å¿…é¡»æ˜¯torch.long
    for edge_type in data.edge_types:
        if data[edge_type].edge_index.dtype != torch.long:
            print(
                f"âŒ DTYPE_ERROR for edge_type '{edge_type}': edge_index is {data[edge_type].edge_index.dtype}, but MUST be torch.long!"
            )
            is_healthy = False

    if is_healthy:
        print("âœ… All edge_index tensors have correct dtype (torch.long).")

    # æ£€æŸ¥2: æ£€æŸ¥æ‰€æœ‰è¾¹ç´¢å¼•æ˜¯å¦è¶Šç•Œ
    for edge_type in data.edge_types:
        src_type, _, dst_type = edge_type

        # æ£€æŸ¥æºèŠ‚ç‚¹
        if data[edge_type].edge_index.numel() > 0:  # ä»…åœ¨æœ‰è¾¹çš„æƒ…å†µä¸‹æ£€æŸ¥
            max_src_id = data[edge_type].edge_index[0].max().item()
            num_src_nodes = data[src_type].num_nodes
            if max_src_id >= num_src_nodes:
                print(
                    f"âŒ BOUNDS_ERROR for edge_type '{edge_type}': Max source ID is {max_src_id}, but node_type '{src_type}' only has {num_src_nodes} nodes!"
                )
                is_healthy = False

            # æ£€æŸ¥ç›®æ ‡èŠ‚ç‚¹
            max_dst_id = data[edge_type].edge_index[1].max().item()
            num_dst_nodes = data[dst_type].num_nodes
            if max_dst_id >= num_dst_nodes:
                print(
                    f"âŒ BOUNDS_ERROR for edge_type '{edge_type}': Max destination ID is {max_dst_id}, but node_type '{dst_type}' only has {num_dst_nodes} nodes!"
                )
                is_healthy = False

    if is_healthy:
        print(
            "âœ… All edge indices are within the bounds of their respective node stores."
        )

    print("--- Deep health check finished. ---")
    return is_healthy


def diagnose_node_features(data: HeteroData) -> bool:
    """
    Performs a deep analysis of node features in a HeteroData object.

    Checks for the presence of dangerous NaN (Not a Number) or Infinity
    values, which are common culprits for Segmentation Faults in C++/CUDA extensions.

    Args:
        data (HeteroData): The graph data object to diagnose.

    Returns:
        bool: True if all features are clean, False otherwise.
    """
    print("\n--- [DIAGNOSTIC] Analyzing node features for invalid values (NaN/inf)...")
    is_clean = True

    for node_type in data.node_types:
        # Check if the node type has features assigned
        if "x" not in data[node_type]:
            print(
                f"âš ï¸  INFO: Node type '{node_type}' has no features ('x' attribute). Skipping."
            )
            continue

        features = data[node_type].x

        # Check for NaN values
        nan_mask = torch.isnan(features)
        if nan_mask.any():
            num_nan = nan_mask.sum().item()
            print(
                f"âŒ FATAL: Found {num_nan} NaN value(s) in features for node_type '{node_type}'!"
            )
            is_clean = False

        # Check for Infinity values
        inf_mask = torch.isinf(features)
        if inf_mask.any():
            num_inf = inf_mask.sum().item()
            print(
                f"âŒ FATAL: Found {num_inf} Infinity value(s) in features for node_type '{node_type}'!"
            )
            is_clean = False

    if is_clean:
        print("âœ… All node features are clean (no NaN/inf found).")

    return is_clean


def sanitize_for_loader(data: HeteroData) -> HeteroData:
    """
    Performs a final, deep sanitization of a HeteroData object to ensure
    all its tensors have a contiguous memory layout before being passed
    to a C++-backed loader.

    Args:
        data (HeteroData): The graph object to sanitize.

    Returns:
        HeteroData: The sanitized graph object.
    """
    print(
        "\n--- [FINAL SANITIZATION] Forcing contiguous memory layout for all tensors..."
    )

    for store in data.stores:
        for key, value in store.items():
            if torch.is_tensor(value):
                # .contiguous() returns a new tensor with contiguous memory if the
                # original is not; otherwise, it returns the original tensor.
                # This is a very cheap operation if the tensor is already contiguous.
                store[key] = value.contiguous()

    print("âœ… All tensors are now memory-contiguous.")
    return data


def validate_entity_list_and_index(
    entity_list: list, entity_to_index_map: dict, entity_type: str, start_index: int = 0
) -> bool:
    """
    ã€å…³é”®è¯Šæ–­ã€‘éªŒè¯ä¸€ä¸ªå®ä½“åˆ—è¡¨å’Œå…¶ç´¢å¼•å­—å…¸ä¹‹é—´çš„é¡ºåºå’Œå†…å®¹æ˜¯å¦ä¸¥æ ¼ä¸€è‡´ã€‚

    æœ¬å‡½æ•°æ‰§è¡Œä¸¤ä¸ªæ ¸å¿ƒæ£€æŸ¥ï¼š
    1.  å†…å®¹ä¸€è‡´æ€§ï¼šåˆ—è¡¨ä¸­çš„æ‰€æœ‰å®ä½“ï¼Œæ˜¯å¦ä¸å­—å…¸çš„é”®å®Œå…¨ç›¸åŒã€‚
    2.  é¡ºåºä¸€è‡´æ€§ï¼šåˆ—è¡¨ä¸­ç¬¬ i ä¸ªå®ä½“ï¼Œå…¶åœ¨å­—å…¸ä¸­å¯¹åº”çš„IDï¼Œæ˜¯å¦ç²¾ç¡®åœ°ç­‰äº i + start_indexã€‚

    Args:
        entity_list (list): å®ä½“çš„æœ‰åºåˆ—è¡¨ (ä¾‹å¦‚ final_proteins_list)ã€‚
        entity_to_index_map (dict): ä»å®ä½“æ˜ å°„åˆ°å…¶å…¨å±€IDçš„å­—å…¸ (ä¾‹å¦‚ prot2index)ã€‚
        entity_type (str): å®ä½“çš„åç§°ï¼Œç”¨äºæ‰“å°æ¸…æ™°çš„æ—¥å¿—ä¿¡æ¯ (ä¾‹å¦‚ "Protein")ã€‚
        start_index (int): è¯¥ç±»å‹å®ä½“çš„å…¨å±€IDèµ·å§‹ç¼–å·ã€‚å¯¹äºdrug/ligandæ˜¯0ï¼Œ
                           å¯¹äºproteinï¼Œæ˜¯drug+ligandçš„æ€»æ•°ã€‚

    Returns:
        bool: å¦‚æœéªŒè¯é€šè¿‡ï¼Œè¿”å›Trueï¼Œå¦åˆ™è¿”å›Falseå¹¶æ‰“å°è¯¦ç»†é”™è¯¯ã€‚
    """
    print(f"--> [DIAGNOSTIC] Validating consistency for '{entity_type}' entities...")

    # 1. å†…å®¹ä¸€è‡´æ€§æ£€æŸ¥
    list_set = set(entity_list)
    dict_keys_set = set(entity_to_index_map.keys())

    if list_set != dict_keys_set:
        print(f"âŒ VALIDATION FAILED for '{entity_type}': Content Mismatch!")
        missing_in_list = dict_keys_set - list_set
        missing_in_dict = list_set - dict_keys_set
        if missing_in_list:
            print(
                f"    - {len(missing_in_list)} items are in the dictionary but NOT in the list."
            )
        if missing_in_dict:
            print(
                f"    - {len(missing_in_dict)} items are in the list but NOT in the dictionary."
            )
        return False

    # 2. é¡ºåºä¸€è‡´æ€§æ£€æŸ¥
    all_ids = sorted(entity_to_index_map.values())
    expected_ids = list(range(start_index, start_index + len(entity_list)))
    if all_ids != expected_ids:
        print(
            f"âŒ VALIDATION FAILED for '{entity_type}': Index values are not contiguous!"
        )
        print(
            f"    - Expected ID range: {start_index} to {start_index + len(entity_list) - 1}"
        )
        print(f"    - Actual IDs found (first 10): {all_ids[:10]}")  # å¯é€‰çš„debugè¾“å‡º
        return False

    for i, entity in enumerate(entity_list):
        expected_id = i + start_index
        actual_id = entity_to_index_map[entity]

        if actual_id != expected_id:
            print(f"âŒ VALIDATION FAILED for '{entity_type}': Order Mismatch!")
            print(
                f"    - At list index {i}, for entity '{str(entity)[:50]}...'"
            )  # æ‰“å°å®ä½“çš„å‰50ä¸ªå­—ç¬¦
            print(f"    - Expected global ID: {expected_id}")
            print(f"    - Actual ID found in dictionary: {actual_id}")
            return False

    print(
        f"âœ… Validation PASSED for '{entity_type}': Content and order are perfectly consistent."
    )
    return True


def validate_embedding_consistency(
    embedding_tensor: torch.Tensor,
    entity_list: list,
    entity_to_index_map: dict,
    entity_type: str,
) -> bool:
    """
    ã€å…³é”®è¯Šæ–­ã€‘éªŒè¯ä¸€ä¸ªé¢„è®¡ç®—çš„åµŒå…¥å¼ é‡ï¼Œä¸å…¶å¯¹åº”çš„å®ä½“åˆ—è¡¨å’Œç´¢å¼•å­—å…¸
    åœ¨ç»´åº¦ã€å†…å®¹å’Œé¡ºåºä¸Šæ˜¯å¦å®Œå…¨ä¸€è‡´ã€‚

    æœ¬å‡½æ•°æ‰§è¡Œä¸‰ä¸ªæ ¸å¿ƒæ£€æŸ¥ï¼š
    1.  ç»´åº¦ä¸€è‡´æ€§ï¼šåµŒå…¥å¼ é‡çš„è¡Œæ•°ï¼Œæ˜¯å¦ä¸å®ä½“åˆ—è¡¨çš„é•¿åº¦å®Œå…¨ç›¸ç­‰ã€‚
    2.  å†…å®¹ä¸€è‡´æ€§ (é—´æ¥)ï¼šå®ä½“åˆ—è¡¨ä¸­çš„å†…å®¹ï¼Œæ˜¯å¦ä¸ç´¢å¼•å­—å…¸çš„é”®å®Œå…¨ä¸€è‡´ã€‚
    3.  é¡ºåºä¸€è‡´æ€§ (é—´æ¥)ï¼šé€šè¿‡æŠ½æ ·æ£€æŸ¥ï¼Œç¡®ä¿åˆ—è¡¨ä¸­çš„å®ä½“é¡ºåºï¼Œä¸å…¶åœ¨
        ç´¢å¼•å­—å…¸ä¸­IDæ‰€å¯¹åº”çš„åµŒå…¥è¡Œå·ï¼Œæ˜¯ä¸¥æ ¼åŒ¹é…çš„ã€‚

    Args:
        embedding_tensor (torch.Tensor): é¢„è®¡ç®—çš„åµŒå…¥å¼ é‡ (ä¾‹å¦‚ protein_embeddings)ã€‚
        entity_list (list): å®ä½“çš„æœ‰åºåˆ—è¡¨ (ä¾‹å¦‚ final_proteins_list)ã€‚
        entity_to_index_map (dict): ä»å®ä½“æ˜ å°„åˆ°å…¶ã€å…¨å±€ã€‘IDçš„å­—å…¸ (ä¾‹å¦‚ prot2index)ã€‚
        entity_type (str): å®ä½“çš„åç§°ï¼Œç”¨äºæ‰“å°æ¸…æ™°çš„æ—¥å¿—ä¿¡æ¯ (ä¾‹å¦‚ "Protein")ã€‚

    Returns:
        bool: å¦‚æœéªŒè¯é€šè¿‡ï¼Œè¿”å›Trueï¼Œå¦åˆ™è¿”å›Falseå¹¶æ‰“å°è¯¦ç»†é”™è¯¯ã€‚
    """
    print(
        f"--> [DIAGNOSTIC] Validating consistency for '{entity_type}' PRE-COMPUTED EMBEDDINGS..."
    )

    # 1. ç»´åº¦ä¸€è‡´æ€§æ£€æŸ¥
    num_embeddings = embedding_tensor.shape[0]
    num_entities_in_list = len(entity_list)

    if num_embeddings != num_entities_in_list:
        print(f"âŒ VALIDATION FAILED for '{entity_type}': Dimension Mismatch!")
        print(f"    - Number of rows in embedding tensor: {num_embeddings}")
        print(f"    - Number of items in entity list: {num_entities_in_list}")
        return False

    # æˆ‘ä»¬å€Ÿç”¨ä¹‹å‰çš„å‡½æ•°æ¥å®Œæˆå†…å®¹å’Œé¡ºåºçš„å†…éƒ¨æ£€æŸ¥
    # æ³¨æ„ï¼šè¿™é‡Œçš„start_indexå¿…é¡»æ˜¯0ï¼Œå› ä¸ºæˆ‘ä»¬æ˜¯åœ¨æ¯”è¾ƒâ€œå±€éƒ¨â€çš„åˆ—è¡¨å’ŒåµŒå…¥
    is_list_and_index_ok = validate_entity_list_and_index(
        entity_list=entity_list,
        entity_to_index_map={
            k: v - min(entity_to_index_map.values())
            for k, v in entity_to_index_map.items()
        },
        entity_type=f"{entity_type} (internal list vs. index)",
        start_index=0,
    )

    if not is_list_and_index_ok:
        print(
            f"âŒ VALIDATION FAILED for '{entity_type}': Internal list/index inconsistency detected."
        )
        return False

    print(
        f"âœ… Validation PASSED for '{entity_type}': Dimensions and internal consistency are correct."
    )
    return True


def validate_data_pipeline_integrity(
    *,  # ä½¿ç”¨æ˜Ÿå·å¼ºåˆ¶æ‰€æœ‰åç»­å‚æ•°éƒ½å¿…é¡»æ˜¯å…³é”®å­—å‚æ•°ï¼Œå¢åŠ å¯è¯»æ€§
    final_smiles_list: list = None,
    final_proteins_list: list = None,
    dl2index: dict = None,
    prot2index: dict = None,
    molecule_embeddings: torch.Tensor = None,
    protein_embeddings: torch.Tensor = None,
):
    """
    ã€é¡¶å±‚è¯Šæ–­åŒ…è£…å™¨ã€‘ä¸€ä¸ªé«˜çº§å‡½æ•°ï¼Œç”¨äºåœ¨æ•°æ®å¤„ç†æµæ°´çº¿çš„ä¸åŒé˜¶æ®µï¼Œ
    éªŒè¯æ‰€æœ‰ç›¸å…³æ•°æ®ç»“æ„ä¹‹é—´çš„å®Œæ•´æ€§å’Œä¸€è‡´æ€§ã€‚

    å®ƒä¼šæ ¹æ®ä¼ å…¥çš„å‚æ•°ï¼Œè‡ªåŠ¨æ‰§è¡Œæ‰€æœ‰ç›¸å…³çš„ä½çº§è¯Šæ–­å‡½æ•°ã€‚
    """
    print("\n" + "=" * 80)
    print(" " * 20 + "RUNNING DATA PIPELINE INTEGRITY VALIDATION")
    print("=" * 80)

    # --- éªŒè¯é˜¶æ®µ 1: å®ä½“åˆ—è¡¨ vs ç´¢å¼•å­—å…¸ ---
    # ä»…å½“ç›¸å…³å‚æ•°è¢«æä¾›æ—¶ï¼Œæ‰æ‰§è¡Œæ­¤æ£€æŸ¥
    if final_smiles_list is not None and dl2index is not None:
        if not validate_entity_list_and_index(
            entity_list=final_smiles_list,
            entity_to_index_map=dl2index,
            entity_type="Molecule (Drug/Ligand)",
            start_index=0,
        ):
            raise ValueError(
                "Stage 1 (Molecules) output failed consistency validation."
            )

    if (
        final_proteins_list is not None
        and prot2index is not None
        and dl2index is not None
    ):
        protein_start_index = len(dl2index)
        if not validate_entity_list_and_index(
            entity_list=final_proteins_list,
            entity_to_index_map=prot2index,
            entity_type="Protein",
            start_index=protein_start_index,
        ):
            raise ValueError("Stage 1 (Proteins) output failed consistency validation.")

    # --- éªŒè¯é˜¶æ®µ 2: åµŒå…¥ vs åˆ—è¡¨/ç´¢å¼• ---
    # ä»…å½“ç›¸å…³å‚æ•°è¢«æä¾›æ—¶ï¼Œæ‰æ‰§è¡Œæ­¤æ£€æŸ¥
    if (
        molecule_embeddings is not None
        and final_smiles_list is not None
        and dl2index is not None
    ):
        if not validate_embedding_consistency(
            embedding_tensor=molecule_embeddings,
            entity_list=final_smiles_list,
            entity_to_index_map=dl2index,
            entity_type="Molecule (Drug/Ligand) Embeddings",
        ):
            raise ValueError(
                "Stage 2 (Molecule Embeddings) failed consistency validation."
            )

    if (
        protein_embeddings is not None
        and final_proteins_list is not None
        and prot2index is not None
    ):
        if not validate_embedding_consistency(
            embedding_tensor=protein_embeddings,
            entity_list=final_proteins_list,
            entity_to_index_map=prot2index,
            entity_type="Protein Embeddings",
        ):
            raise ValueError(
                "Stage 2 (Protein Embeddings) failed consistency validation."
            )

    print("=" * 80)
    print(" " * 22 + "âœ… PIPELINE INTEGRITY VALIDATION PASSED âœ…")
    print("=" * 80 + "\n")


# å¯ä»¥åœ¨æ–‡ä»¶é¡¶éƒ¨å®šä¹‰ä¸€äº›é¢œè‰²ï¼Œè®©è¾“å‡ºæ›´é†’ç›®
class bcolors:
    HEADER = "\033[95m"
    OKBLUE = "\033[94m"
    OKCYAN = "\033[96m"
    OKGREEN = "\033[92m"
    WARNING = "\033[93m"
    FAIL = "\033[91m"
    ENDC = "\033[0m"
    BOLD = "\033[1m"
    UNDERLINE = "\033[4m"


def validate_authoritative_dti_file(
    config: DictConfig, df: Optional[pd.DataFrame] = None
):
    """
    ä¸€ä¸ªé€šç”¨çš„ã€ä¸¥æ ¼çš„éªŒè¯å‡½æ•°ï¼Œç”¨äºå®¡æŸ¥ç”±ä»»ä½•æ•°æ®å¤„ç†æµæ°´çº¿ç”Ÿæˆçš„
    æœ€ç»ˆæƒå¨DTIäº¤äº’æ–‡ä»¶(ä¾‹å¦‚ full.csv)ã€‚

    Args:
        config (DictConfig): å®éªŒçš„å®Œæ•´é…ç½®å¯¹è±¡ã€‚
        df (Optional[pd.DataFrame]): ä¸€ä¸ªå¯é€‰çš„ã€å·²åŠ è½½çš„DataFrameã€‚
                                      å¦‚æœä¸ºNone,å‡½æ•°å°†ä»configæŒ‡å®šçš„è·¯å¾„åŠ è½½ã€‚

    Raises:
        AssertionError: å¦‚æœæ£€æµ‹åˆ°ä»»ä½•ä¸¥é‡çš„æ•°æ®è´¨é‡é—®é¢˜ã€‚
    """
    print("\n" + "=" * 80)
    print(
        f"{bcolors.HEADER}{bcolors.BOLD}"
        + " " * 18
        + "å¼€å§‹æ‰§è¡Œæƒå¨DTIæ–‡ä»¶è´¨é‡æ£€éªŒæµç¨‹"
        + f"{bcolors.ENDC}"
    )
    print("=" * 80)

    # --- 1. åŠ è½½æ•°æ® ---
    if df is None:
        try:
            file_path = rt.get_path(config, "raw.dti_interactions")
            print(f"æ­£åœ¨åŠ è½½æ–‡ä»¶: {file_path}")
            df = pd.read_csv(file_path)
            print(
                f"--> {bcolors.OKGREEN}æ–‡ä»¶åŠ è½½æˆåŠŸã€‚å…± {len(df)} è¡Œè®°å½•ã€‚{bcolors.ENDC}"
            )
        except FileNotFoundError:
            print(
                f"âŒ {bcolors.FAIL}è‡´å‘½é”™è¯¯: æ‰¾ä¸åˆ°æŒ‡å®šçš„DTIæ–‡ä»¶: {file_path}{bcolors.ENDC}"
            )
            raise
    else:
        print("--> ä½¿ç”¨å·²ä¼ å…¥çš„DataFrameè¿›è¡Œæ£€éªŒã€‚")

    # --- 2. æ¨¡å¼å’Œç»“æ„éªŒè¯ ---
    print("\n" + "-" * 30 + " 1. æ¨¡å¼ä¸ç»“æ„éªŒè¯ " + "-" * 29)
    required_columns = {"PubChem_CID", "UniProt_ID", "SMILES", "Sequence", "Label"}
    actual_columns = set(df.columns)
    assert required_columns.issubset(actual_columns), (
        f"âŒ {bcolors.FAIL}éªŒè¯å¤±è´¥: æ–‡ä»¶ç¼ºå°‘å¿…éœ€çš„åˆ—ã€‚éœ€è¦: {required_columns}, å®é™…: {actual_columns}{bcolors.ENDC}"
    )
    print(f"âœ… {bcolors.OKGREEN}åˆ—å®Œæ•´æ€§: æ‰€æœ‰å¿…éœ€åˆ—å‡å­˜åœ¨ã€‚{bcolors.ENDC}")

    assert pd.api.types.is_integer_dtype(df["PubChem_CID"]), (
        f"âŒ {bcolors.FAIL}éªŒè¯å¤±è´¥: 'PubChem_CID' åˆ—åº”ä¸ºæ•´æ•°ç±»å‹ã€‚{bcolors.ENDC}"
    )
    assert pd.api.types.is_integer_dtype(df["Label"]), (
        f"âŒ {bcolors.FAIL}éªŒè¯å¤±è´¥: 'Label' åˆ—åº”ä¸ºæ•´æ•°ç±»å‹ã€‚{bcolors.ENDC}"
    )
    print(f"âœ… {bcolors.OKGREEN}æ•°æ®ç±»å‹: å…³é”®åˆ—çš„æ•°æ®ç±»å‹æ­£ç¡®ã€‚{bcolors.ENDC}")

    # --- 3. æ•°æ®å”¯ä¸€æ€§éªŒè¯ ---
    print("\n" + "-" * 30 + " 2. æ•°æ®å”¯ä¸€æ€§éªŒè¯ " + "-" * 29)
    duplicates = df.duplicated(subset=["PubChem_CID", "UniProt_ID"]).sum()
    assert duplicates == 0, (
        f"âŒ {bcolors.FAIL}éªŒè¯å¤±è´¥: åœ¨ ('PubChem_CID', 'UniProt_ID') ä¸Šå‘ç° {duplicates} æ¡é‡å¤è®°å½•ã€‚{bcolors.ENDC}"
    )
    print(
        f"âœ… {bcolors.OKGREEN}äº¤äº’å¯¹å”¯ä¸€æ€§: æ‰€æœ‰ (è¯ç‰©, é¶ç‚¹) å¯¹éƒ½æ˜¯å”¯ä¸€çš„ã€‚{bcolors.ENDC}"
    )

    # --- 4. å†…å®¹æœ‰æ•ˆæ€§éªŒè¯ ---
    print("\n" + "-" * 30 + " 3. å†…å®¹æœ‰æ•ˆæ€§éªŒè¯ " + "-" * 30)

    # a. SMILES æœ‰æ•ˆæ€§
    sample_size = min(len(df), 5000)  # æœ€å¤šæ£€æŸ¥5000ä¸ªæ ·æœ¬ï¼Œé¿å…å¤§æ•°æ®é›†ä¸Šè€—æ—¶è¿‡é•¿
    invalid_smiles_count = 0
    sampled_smiles = df["SMILES"].sample(
        n=sample_size, random_state=config.runtime.seed
    )
    for smiles in tqdm(sampled_smiles, desc="æ£€éªŒSMILESæœ‰æ•ˆæ€§"):
        if Chem.MolFromSmiles(smiles) is None:
            invalid_smiles_count += 1

    invalidation_rate = (invalid_smiles_count / sample_size) * 100
    assert invalidation_rate < 0.1, (
        f"âŒ {bcolors.FAIL}éªŒè¯å¤±è´¥: SMILESæœ‰æ•ˆæ€§è¿‡ä½ã€‚åœ¨{sample_size}ä¸ªæ ·æœ¬ä¸­å‘ç° {invalid_smiles_count} ({invalidation_rate:.2f}%) ä¸ªæ— æ•ˆSMILESã€‚{bcolors.ENDC}"
    )
    print(
        f"âœ… {bcolors.OKGREEN}SMILESæœ‰æ•ˆæ€§: åœ¨{sample_size}ä¸ªæ ·æœ¬ä¸­ï¼Œæ— æ•ˆæ¯”ä¾‹ä¸º {invalidation_rate:.2f}% (é€šè¿‡)ã€‚{bcolors.ENDC}"
    )

    # b. UniProt ID æ ¼å¼
    uniprot_pattern = re.compile(
        r"([OPQ][0-9][A-Z0-9]{3}[0-9]|[A-NR-Z][0-9]([A-Z][A-Z0-9]{2}[0-9]){1,2})"
    )
    invalid_uniprot_ids = df[~df["UniProt_ID"].astype(str).str.match(uniprot_pattern)]
    assert len(invalid_uniprot_ids) == 0, (
        f"âŒ {bcolors.FAIL}éªŒè¯å¤±è´¥: å‘ç° {len(invalid_uniprot_ids)} ä¸ªä¸ç¬¦åˆæ ‡å‡†æ ¼å¼çš„UniProt IDã€‚ä¾‹å¦‚: {invalid_uniprot_ids['UniProt_ID'].head().tolist()}{bcolors.ENDC}"
    )
    print(f"âœ… {bcolors.OKGREEN}UniProt IDæ ¼å¼: æ‰€æœ‰IDå‡ç¬¦åˆæ ‡å‡†æ ¼å¼ã€‚{bcolors.ENDC}")

    # a. å®šä¹‰åˆæ³•çš„æ°¨åŸºé…¸å­—ç¬¦é›†
    amino_acids = "ACDEFGHIKLMNPQRSTVWYU"

    # b. æ„å»ºæŸ¥æ‰¾éæ³•å­—ç¬¦çš„æ­£åˆ™è¡¨è¾¾å¼
    invalid_char_pattern = f"[^{amino_acids}]"

    # c. æ‰¾å‡ºæ‰€æœ‰åŒ…å«éæ³•å­—ç¬¦çš„åºåˆ—çš„DataFrame
    invalid_seq_df = df[
        df["Sequence"]
        .str.upper()
        .str.contains(invalid_char_pattern, regex=True, na=False)
    ]

    # d. æ£€æŸ¥æ˜¯å¦å­˜åœ¨æ— æ•ˆåºåˆ—
    if not invalid_seq_df.empty:
        num_invalid = len(invalid_seq_df)
        print(
            f"âŒ {bcolors.FAIL}éªŒè¯å¤±è´¥: å‘ç° {num_invalid} æ¡è›‹ç™½è´¨åºåˆ—åŒ…å«éæ³•å­—ç¬¦ã€‚{bcolors.ENDC}"
        )
        print("--- æ— æ•ˆåºåˆ—æ ·æœ¬ (å‰5æ¡): ---")
        # ä½¿ç”¨ .to_string() ä¿è¯æ‰“å°å†…å®¹å¯¹é½
        print(invalid_seq_df[["Sequence"]].head().to_string())
        print("-" * 30)
        # æŠ›å‡ºæ›´æ˜ç¡®çš„é”™è¯¯
        raise ValueError(f"æ•°æ®é›†ä¸­å­˜åœ¨ {num_invalid} æ¡æ— æ•ˆçš„è›‹ç™½è´¨åºåˆ—ã€‚")
    else:
        print(
            f"âœ… {bcolors.OKGREEN}è›‹ç™½è´¨åºåˆ—å†…å®¹: æ‰€æœ‰åºåˆ—å‡ç”±åˆæ³•çš„æ°¨åŸºé…¸å­—ç¬¦ç»„æˆã€‚{bcolors.ENDC}"
        )

    # --- 5. æœ€ç»ˆæ€»ç»“ ---
    print("\n" + "=" * 80)
    print(
        f"{bcolors.OKGREEN}{bcolors.BOLD}"
        + " " * 25
        + "âœ… æ‰€æœ‰éªŒè¯é¡¹ç›®å‡å·²é€šè¿‡ âœ…"
        + f"{bcolors.ENDC}"
    )
    print("=" * 80)


# å®šä¹‰ä¸€ä¸ªç»“æ„åŒ–çš„è¿”å›ç±»å‹ï¼Œè®©ç»“æœæ›´æ¸…æ™°
ValidationResult = namedtuple("ValidationResult", ["status", "message"])

# --- æ ¸å¿ƒéªŒè¯å‡½æ•° ---


def validate_pubchem_entry(cid: int, local_smiles: str) -> ValidationResult:
    """
    é€šè¿‡PubChem PUG REST APIæŸ¥è¯¢ç»™å®šçš„CIDï¼Œå¹¶å°†å…¶è§„èŒƒSMILESä¸æœ¬åœ°SMILESè¿›è¡Œæ¯”è¾ƒã€‚

    Args:
        cid (int): PubChem Compound ID.
        local_smiles (str): æ•°æ®é›†ä¸­ä¸è¯¥CIDå…³è”çš„SMILESå­—ç¬¦ä¸²ã€‚

    Returns:
        ValidationResult: åŒ…å« 'MATCH', 'MISMATCH', æˆ– 'API_ERROR' çŠ¶æ€çš„ç»“æœã€‚
    """
    # PubChem è¦æ±‚æ¯ç§’è¯·æ±‚ä¸è¶…è¿‡5æ¬¡ã€‚åœ¨æ¯ä¸ªè¯·æ±‚å‰æš‚åœä¸€ä¸‹æ¥éµå®ˆè§„åˆ™ã€‚
    time.sleep(0.25)
    url = f"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/cid/{cid}/property/IsomericSMILES/TXT"
    # url = f"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/cid/{cid}/property/CanonicalSMILES/TXT"
    try:
        response = requests.get(url, timeout=15)
        response.raise_for_status()  # æ£€æŸ¥HTTPé”™è¯¯ (å¦‚ 404, 500)

        api_smiles = response.text.strip()

        local_mol = Chem.MolFromSmiles(local_smiles)
        api_mol = Chem.MolFromSmiles(api_smiles)

        if not local_mol:
            return ValidationResult("LOCAL_INVALID", "Local SMILES is invalid.")
        if not api_mol:
            return ValidationResult("API_INVALID", "API SMILES is invalid.")

        # --- ã€æ ¸å¿ƒä¿®å¤ã€‘ ---
        # ç›´æ¥å°†åˆ†å­å¯¹è±¡ local_mol å’Œ api_mol ä¼ ç»™æŒ‡çº¹å‡½æ•°
        local_fp = Chem.RDKFingerprint(local_mol)
        api_fp = Chem.RDKFingerprint(api_mol)

        if local_fp == api_fp:
            return ValidationResult("MATCH", "Molecules are chemically equivalent.")
        else:
            # åªæœ‰åœ¨ä¸åŒ¹é…æ—¶ï¼Œæˆ‘ä»¬æ‰ä¸ºäº†ç”ŸæˆæŠ¥å‘Šè€Œåˆ›å»ºè§„èŒƒSMILESå­—ç¬¦ä¸²
            local_canonical_str = Chem.MolToSmiles(local_mol, canonical=True)
            api_canonical_str = Chem.MolToSmiles(api_mol, canonical=True)
            msg = f"Molecules are different. Local: '{local_canonical_str}' vs API: '{api_canonical_str}'"
            return ValidationResult("MISMATCH", msg)

    except requests.exceptions.RequestException as e:
        return ValidationResult("API_ERROR", str(e))


def validate_uniprot_entry(pid: str, local_sequence: str) -> ValidationResult:
    """
    é€šè¿‡UniProt APIæŸ¥è¯¢ç»™å®šçš„è›‹ç™½è´¨IDï¼Œå¹¶å°†å…¶åºåˆ—ä¸æœ¬åœ°åºåˆ—è¿›è¡Œæ¯”è¾ƒã€‚

    Args:
        pid (str): UniProt Primary Accession ID.
        local_sequence (str): æ•°æ®é›†ä¸­ä¸è¯¥PIDå…³è”çš„è›‹ç™½è´¨åºåˆ—ã€‚

    Returns:
        ValidationResult: åŒ…å« 'MATCH', 'MISMATCH', æˆ– 'API_ERROR' çŠ¶æ€çš„ç»“æœã€‚
    """
    url = f"https://rest.uniprot.org/uniprotkb/{pid}.fasta"
    try:
        response = requests.get(url, timeout=15)
        response.raise_for_status()

        # è§£æFASTAæ ¼å¼
        lines = response.text.strip().split("\n")
        api_sequence = "".join(lines[1:])
        # ã€æ ¸å¿ƒä¿®å¤ã€‘: åœ¨æ¯”è¾ƒå‰å¯¹ä¸¤ä¸ªå­—ç¬¦ä¸²éƒ½ä½¿ç”¨ .strip()
        local_seq_clean = local_sequence.strip().upper()
        api_seq_clean = api_sequence.strip().upper()

        if local_seq_clean == api_seq_clean:
            return ValidationResult("MATCH", "Sequences successfully matched.")
        else:
            # ç°åœ¨ï¼Œä»»ä½•ä¸åŒ¹é…éƒ½æ˜¯çœŸå®çš„å†…å®¹å·®å¼‚
            import difflib

            if len(local_seq_clean) == len(api_seq_clean):
                # ä½¿ç”¨difflibæ‰¾å‡ºå…·ä½“å­—ç¬¦å·®å¼‚
                diff = list(difflib.ndiff(local_seq_clean, api_seq_clean))
                # è¿‡æ»¤å‡ºæœ‰å·®å¼‚çš„éƒ¨åˆ†ï¼Œå¹¶ä½¿å…¶æ›´å¯è¯»
                diff_parts = [
                    d for d in diff if d.startswith("+ ") or d.startswith("- ")
                ]
                # å°†ä¾‹å¦‚ ['- A', '+ B', '- C', '+ D'] å˜æˆ "A->B, C->D"
                readable_diff = []
                # å‡è®¾å·®å¼‚æ€»æ˜¯æˆå¯¹å‡ºç°
                for i in range(0, len(diff_parts), 2):
                    if i + 1 < len(diff_parts):
                        readable_diff.append(
                            f"{diff_parts[i][-1]}->{diff_parts[i + 1][-1]}"
                        )
                msg = f"Content mismatch: {', '.join(readable_diff)[:100]}"
            elif local_seq_clean in api_seq_clean or api_seq_clean in local_seq_clean:
                msg = f"Local sequence is a SUBSTRING of API sequence (or vice versa). Local={len(local_seq_clean)}, API={len(api_seq_clean)}"
                return ValidationResult("MATCH_SUBSTRING", msg)
            else:
                msg = f"Length mismatch: Local={len(local_seq_clean)}, API={len(api_seq_clean)}"

            return ValidationResult("MISMATCH", msg)

    except requests.exceptions.RequestException as e:
        return ValidationResult("API_ERROR", str(e))


# --- ä¸»åè°ƒä¸æŠ¥å‘Šå‡½æ•° ---


def _print_validation_report(
    title: str, results: list[ValidationResult], sample_df: pd.DataFrame, id_col: str
):
    """ä¸€ä¸ªè¾…åŠ©å‡½æ•°ï¼Œç”¨äºæ‰“å°æ ¼å¼åŒ–çš„æŠ¥å‘Šã€‚"""
    print("\n" + "=" * 30)
    print(f" {title} Validation Report")
    print("=" * 30)

    counts = Counter(r.status for r in results)
    total = len(results)

    print(f"  - Total Samples: {total}")
    for status, count in counts.items():
        percentage = (count / total) * 100
        print(f"  - {status:<10}: {count:>4} ({percentage:.1f}%)")

    mismatches = [
        (res.message, row[id_col])
        for res, (_, row) in zip(results, sample_df.iterrows())
        if res.status == "MISMATCH"
    ]

    if mismatches:
        print("\n--- Mismatch Details (up to 5) ---")
        for i, (msg, item_id) in enumerate(mismatches[:5]):
            print(f"{i + 1}. ID: {item_id}")
            print(f"   Reason: {msg}")
    print("=" * 30)


def run_online_validation(
    df: pd.DataFrame, n_samples: int = 200, n_jobs: int = 4, random_state: int = 42
):
    """
    ä»ç»™å®šçš„DataFrameä¸­æŠ½æ ·ï¼Œå¹¶è¡Œæ‰§è¡Œå¯¹PubChemå’ŒUniProtçš„åœ¨çº¿éªŒè¯ï¼Œå¹¶æ‰“å°æ€»ç»“æŠ¥å‘Šã€‚

    Args:
        df (pd.DataFrame): åŒ…å« 'PubChem_CID', 'SMILES', 'UniProt_ID', 'Sequence' åˆ—çš„DataFrameã€‚
        n_samples (int): è¦éšæœºæŠ½å–çš„æ ·æœ¬æ•°é‡ã€‚
        n_jobs (int): ç”¨äºå¹¶è¡ŒAPIè¯·æ±‚çš„ä½œä¸šæ•°ã€‚
                       æ³¨æ„ï¼šPubChemæœ‰é€Ÿç‡é™åˆ¶ï¼Œè¿‡é«˜çš„n_jobså¯èƒ½æ— ç›Šã€‚
        random_state (int): ç”¨äºæŠ½æ ·çš„éšæœºç§å­ï¼Œä»¥ä¿è¯ç»“æœå¯å¤ç°ã€‚
    """
    print(f"\nğŸš€ Starting online validation for {n_samples} random samples...")

    if n_samples > len(df):
        print(
            f"Warning: n_samples ({n_samples}) is larger than DataFrame size ({len(df)}). Validating all entries."
        )
        n_samples = len(df)

    sample_df = df.sample(n=n_samples, random_state=random_state)

    with Parallel(n_jobs=n_jobs) as parallel:
        # --- PubChem Validation ---
        print("\n[Phase 1/2] Querying PubChem API for SMILES validation...")
        pubchem_results = parallel(
            delayed(validate_pubchem_entry)(row.PubChem_CID, row.SMILES)
            for _, row in tqdm(
                sample_df.iterrows(), total=len(sample_df), desc="PubChem Checks"
            )
        )

        # --- UniProt Validation ---
        print("\n[Phase 2/2] Querying UniProt API for Sequence validation...")
        uniprot_results = parallel(
            delayed(validate_uniprot_entry)(row.UniProt_ID, row.Sequence)
            for _, row in tqdm(
                sample_df.iterrows(), total=len(sample_df), desc="UniProt Checks"
            )
        )

    # --- Generate Reports ---
    _print_validation_report(
        "PubChem CID vs SMILES", pubchem_results, sample_df, "PubChem_CID"
    )
    _print_validation_report(
        "UniProt ID vs Sequence", uniprot_results, sample_df, "UniProt_ID"
    )
