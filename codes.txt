# src/gtopdb_proc.py

import pandas as pd
import sys
from tqdm import tqdm
import time
from pathlib import Path
import requests
import re
import yaml
from research_template import get_path


def load_config(config_path="config.yaml"):
    """Loads the YAML config file from the project root."""
    project_root = Path(__file__).parent.parent
    with open(project_root / config_path, "r") as f:
        return yaml.safe_load(f)


# Load config at the start of the script
config = load_config()


def is_valid_uniprot_accession(accession):
    """
    使用正则表达式，快速检查一个ID是否符合UniProt Accession的典型格式。
    这是一个简化版检查，但能过滤掉大部分非Accession的ID。
    """
    # 典型的UniProt Accession格式: e.g., P12345, Q9Y261, A0A024R1R8
    # 规则: 字母开头，后面跟5个或更多数字/字母
    # [OPQ][0-9][A-Z0-9]{3}[0-9] | [A-NR-Z][0-9]([A-Z][A-Z0-9]{2}[0-9]){1,2}
    # 我们用一个简化的版本
    pattern = re.compile(
        r"^[A-NR-Z][0-9][A-Z][A-Z0-9]{2}[0-9]$|^[OPQ][0-9][A-Z0-9]{3}[0-9]$",
        re.IGNORECASE,
    )
    return bool(pattern.match(str(accession)))


def fetch_sequences_from_uniprot(uniprot_ids):
    """
    【最终版】
    根据UniProt ID列表，使用requests库直接调用UniProt官方API，批量获取蛋白质序列。
    这是一个更现代、更健壮的实现。
    """
    valid_ids = sorted(
        [uid for uid in set(uniprot_ids) if is_valid_uniprot_accession(uid)]
    )
    invalid_ids = set(uniprot_ids) - set(valid_ids)
    if invalid_ids:
        print(
            f"Warning: Skipped {len(invalid_ids)} IDs with non-standard format. Examples: {list(invalid_ids)[:5]}"
        )

    print(f"Fetching sequences for {len(valid_ids)} valid UniProt IDs...")

    base_url = "https://rest.uniprot.org/uniprotkb/stream"
    sequences_map = {}
    chunk_size = 100

    for i in tqdm(range(0, len(valid_ids), chunk_size), desc="Querying UniProt API"):
        chunk = valid_ids[i : i + chunk_size]

        params = {
            "query": " OR ".join(f"(accession:{acc})" for acc in chunk),
            "format": "fasta",
        }

        try:
            response = requests.get(base_url, params=params)

            # 检查请求是否成功
            if response.status_code == 200:
                fasta_text = response.text

                # 解析返回的FASTA文本 (这部分逻辑和之前一样)
                for entry in fasta_text.strip().split(">"):
                    if not entry.strip():
                        continue
                    lines = entry.strip().split("\n")
                    header = lines[0]
                    seq = "".join(lines[1:])

                    try:
                        uid = header.split("|")[1]
                        sequences_map[uid] = seq
                    except IndexError:
                        print(
                            f"\nWarning: Could not parse UniProt ID from header: '{header}'"
                        )
            elif response.status_code == 400 and len(chunk) > 1:
                print(
                    f"\nWarning: Batch request failed (400 Bad Request). Switching to individual retry for {len(chunk)} IDs..."
                )
                for single_id in tqdm(chunk, desc="Retrying individually", leave=False):
                    single_params = {
                        "query": f"(accession:{single_id})",
                        "format": "fasta",
                    }
                    try:
                        single_response = requests.get(
                            base_url, params=single_params, timeout=10
                        )
                        if single_response.status_code == 200:
                            s_fasta = single_response.text
                            if s_fasta and s_fasta.startswith(">"):
                                s_lines = s_fasta.strip().split("\n")
                                s_header, s_seq = s_lines[0], "".join(s_lines[1:])
                                s_uid = s_header.split("|")[1]
                                sequences_map[s_uid] = s_seq
                        else:
                            print(
                                f"-> Failed for single ID: {single_id} (Status: {single_response.status_code})"
                            )
                    except Exception as single_e:
                        print(
                            f"-> Network/Parse error for single ID {single_id}: {single_e}"
                        )
                    time.sleep(0.2)  # 单个查询之间也稍作等待

            else:
                # 如果请求失败，打印出错误状态码
                print(
                    f"\nWarning: UniProt API request failed for chunk starting with {chunk[0]}. Status code: {response.status_code}"
                )

        except requests.exceptions.RequestException as e:
            # 捕获网络层面的错误
            print("\n--- NETWORK ERROR during UniProt fetch ---")
            print(f"Error: {e}")
            print("------------------------------------------")

        time.sleep(1)  # 保持API礼仪

    print(f"-> FINAL: Successfully fetched {len(sequences_map)} sequences.")
    return sequences_map


def process_gtopdb_data():
    """
    解析Guide to PHARMACOLOGY的原始数据文件,提取内源性配体-靶点相互作用，
    并生成可用于下游异构网络构建的干净文件。

    参数:
        input_dir (Path): 存放下载的GtoPdb原始CSV文件的目录路径。
        output_dir (Path): 存放处理后输出文件的目录路径。
    """
    print("==========================================================")
    print("  Starting Guide to PHARMACOLOGY Data Processing Pipeline  ")
    print("==========================================================")

    # --- 1. 加载核心数据文件 ---

    print("\n[Step 1/4] Loading raw data files ")
    try:
        interactions_df = pd.read_csv(
            get_path(config, "gtopdb.raw.interactions"), low_memory=False, comment="#"
        )
        ligands_df = pd.read_csv(
            get_path(config, "gtopdb.raw.ligands"), low_memory=False, comment="#"
        )
    except FileNotFoundError as e:
        print(f"Error: Raw CSV file not found! {e}")
        print(
            "Please ensure 'interactions.csv' and 'ligands.csv' are in the specified input directory."
        )
        sys.exit(1)  # 退出程序

    print(f"-> Loaded {len(interactions_df)} total interactions.")
    print(f"-> Loaded {len(ligands_df)} total ligands.")

    # --- 2. 筛选内源性配体相互作用 ---
    print("\n[Step 2/4] Filtering for high-quality endogenous ligand interactions...")

    # 核心筛选条件：'endogenous' 列为 True
    endogenous_interactions = interactions_df[interactions_df["Endogenous"]].copy()
    print(
        f"-> Found {len(endogenous_interactions)} interactions involving endogenous ligands."
    )

    # [MODIFIED] 现在我们依赖 'Original Affinity Median nm'，所以它是必需的
    required_cols = ["Target UniProt ID", "Ligand ID", "Original Affinity Median nm"]
    endogenous_interactions.dropna(subset=required_cols, inplace=True)
    print(
        f"-> After cleaning (removing entries with missing IDs or standardized affinity), {len(endogenous_interactions)} interactions remain."
    )

    # --- [MODIFIED] Affinity-based Filtering ---
    try:
        affinity_threshold = config["params"]["gtopdb"]["max_affinity_nM"]
        print(
            f"--> Applying affinity threshold: retaining interactions with 'Original Affinity Median nm' <= {affinity_threshold} nM."
        )

        # 确保亲和力列是数值类型
        endogenous_interactions["Original Affinity Median nm"] = pd.to_numeric(
            endogenous_interactions["Original Affinity Median nm"], errors="coerce"
        )
        endogenous_interactions.dropna(
            subset=["Original Affinity Median nm"], inplace=True
        )

        original_count = len(endogenous_interactions)

        # [MODIFIED] 使用正确的列进行筛选
        endogenous_interactions = endogenous_interactions[
            endogenous_interactions["Original Affinity Median nm"] <= affinity_threshold
        ].copy()

        print(
            f"--> After affinity filtering, {len(endogenous_interactions)} of {original_count} interactions were retained."
        )

    except KeyError:
        print(
            "--> WARNING: Affinity threshold not found in config.yaml. Skipping affinity-based filtering."
        )
    # --- [NEW] End: Affinity-based Filtering ---
    # --- 3. 提取并清洗配体信息 (获取SMILES) ---
    print("\n[Step 3/4] Extracting SMILES for the relevant endogenous ligands...")

    # 获取我们需要的配体的唯一ID列表
    relevant_ligand_ids = endogenous_interactions["Ligand ID"].unique()

    # 从大的配体表中，只筛选出我们需要的配体信息
    relevant_ligands = ligands_df[
        ligands_df["Ligand ID"].isin(relevant_ligand_ids)
    ].copy()

    # 清洗配体数据：只保留有SMILES和PubChem CID的记录
    relevant_ligands.dropna(subset=["SMILES", "PubChem CID"], inplace=True)
    relevant_ligands["PubChem CID"] = relevant_ligands["PubChem CID"].astype(
        int
    )  # 确保CID是整数
    print(f"-> Found SMILES for {len(relevant_ligands)} unique endogenous ligands.")

    # --- 4. 合并信息并保存为最终的输出文件 ---
    print("\n[Step 4/4] Merging data and saving to output files...")

    # 将相互作用数据与配体数据通过 'ligand_id' 连接起来
    final_df = pd.merge(
        endogenous_interactions,
        relevant_ligands,
        left_on="Ligand ID",
        right_on="Ligand ID",
    )
    print("\n[NEW Step] Fetching protein sequences for all relevant targets...")

    # 1. 从合并后的数据中，收集所有需要查询序列的唯一UniProt ID
    #    先处理'|'分隔符，然后展开列表，最后取唯一值
    all_target_uniprot_ids = (
        final_df["Target UniProt ID"]
        .str.split("|")
        .explode()
        .dropna()
        .unique()
        .tolist()
    )

    # 2. 调用我们写的函数，去UniProt“补全”序列信息
    uniprot_to_sequence_map = fetch_sequences_from_uniprot(all_target_uniprot_ids)

    # 3. 将查询到的序列映射回我们的主DataFrame
    #    我们只关心主要ID（第一个ID）的序列
    main_uniprot_id = final_df["Target UniProt ID"].str.split("|").str[0]
    final_df["target_sequence"] = main_uniprot_id.map(uniprot_to_sequence_map)

    # 4. 清洗：丢弃那些因为某些原因没能查到序列的记录
    original_count = len(final_df)
    final_df.dropna(subset=["target_sequence"], inplace=True)
    print(
        f"-> Found sequences for {len(final_df)} out of {original_count} interactions. Proceeding with these."
    )

    # --- 5. 保存最终的输出文件 (现在输出的文件信息更完整了) ---
    print("\n[Final Step] Saving enriched data to output files...")

    # a) 保存新的 P-L 边文件，现在包含【序列】而不是ID
    # 文件格式: protein_sequence, ligand_smiles, affinity_median
    output_edges = final_df[["target_sequence", "SMILES", "Affinity Median"]].copy()
    output_edges.rename(
        columns={"target_sequence": 0, "SMILES": 1, "Affinity Median": 2}, inplace=True
    )

    output_edge_path = get_path(config, "gtopdb.processed.interactions")
    output_edges.to_csv(output_edge_path, index=False, header=False)
    print(
        f"-> Successfully saved {len(output_edges)} Protein-Ligand edges (with sequences and SMILES) to: {output_edge_path}"
    )

    # b) 保存新的配体信息文件 (可以保持不变，也可以只保存一次)
    output_ligands = final_df[["PubChem CID", "SMILES"]].drop_duplicates()
    output_ligands.rename(columns={"PubChem CID": 0, "SMILES": 1}, inplace=True)
    output_ligand_path = get_path(config, "gtopdb.processed.ligands")
    output_ligands.to_csv(output_ligand_path, index=False, header=False)
    print(
        f"-> Successfully saved info for {len(output_ligands)} Ligand nodes to: {output_ligand_path}"
    )

    # c) (推荐) 另外保存一份蛋白质的 "ID-序列" 对应表，以备后用
    output_proteins = final_df[
        ["Target UniProt ID", "target_sequence"]
    ].drop_duplicates()
    output_proteins["Target UniProt ID"] = (
        output_proteins["Target UniProt ID"].str.split("|").str[0]
    )
    output_proteins.rename(
        columns={"Target UniProt ID": 0, "target_sequence": 1}, inplace=True
    )
    output_protein_path = get_path(config, "gtopdb.processed.proteins")
    output_proteins.to_csv(output_protein_path, index=False, header=False)
    print(
        f"-> Successfully saved info for {len(output_proteins)} Protein nodes to: {output_protein_path}"
    )

    print("\n==========================================================")
    print("  GtoPdb Processing Finished Successfully!  ")
    print("==========================================================")


if __name__ == "__main__":
    # --- 执行主函数 ---
    process_gtopdb_data()
import random
import numpy as np
import pandas as pd
import pickle as pkl
from research_template import get_path, check_files_exist
import research_template as rt
from omegaconf import DictConfig
from pathlib import Path
from tqdm import tqdm
from features.feature_extractors import (
    MoleculeGCN,
    ProteinGCN,
    extract_molecule_features,
    extract_protein_features,
    canonicalize_smiles,
)
from features.similarity_calculators import (
    calculate_drug_similarity,
    calculate_protein_similarity,
)
from sklearn.model_selection import KFold, StratifiedKFold, train_test_split


def process_data(config: DictConfig):
    restart_flag = config.runtime.get("force_restart", False)
    full_df = pd.read_csv(get_path(config, "raw.dti_interactions"))
    (
        drug2index,
        ligand2index,
        prot2index,
        dl2index,
        final_smiles_list,
        final_proteins_list,
    ) = _stage_1_load_and_index_entities(config, full_df, restart_flag)

    dl_similarity_matrix, prot_similarity_matrix, _ = (
        _stage_2_generate_features_and_similarities(
            config,
            drug2index,
            ligand2index,
            prot2index,
            final_smiles_list,
            final_proteins_list,
            restart_flag,
        )
    )
    _stage_3_split_data_and_build_graphs(
        config,
        full_df,
        drug2index=drug2index,
        prot2index=prot2index,
        ligand2index=ligand2index,
        dl2index=dl2index,
        dl_sim_matrix=dl_similarity_matrix,
        prot_sim_matrix=prot_similarity_matrix,
    )


def _stage_1_load_and_index_entities(
    config: DictConfig, full_df: pd.DataFrame, restart_flag: bool = False
) -> tuple:
    # 1a. 加载基础数据集的实体
    data_config = config["data"]
    print("--- [Stage 1a] Loading base entities from full_df.csv ---")
    checkpoint_files_dict = {
        "drug": "processed.indexes.drug",
        "ligand": "processed.indexes.ligand",
        "protein": "processed.indexes.protein",
        "nodes": "processed.nodes_metadata",
    }
    if not check_files_exist(config, *checkpoint_files_dict.values()) or restart_flag:
        # 使用 .unique().tolist() 更高效
        unique_smiles = full_df["SMILES"].dropna().unique()
        base_drugs_list = [
            canonicalize_smiles(s)
            for s in tqdm(unique_smiles, desc="Canonicalizing Drug SMILES", leave=False)
        ]
        base_drugs_list = [s for s in base_drugs_list if s is not None]
        base_proteins_list = full_df["Protein"].unique().tolist()

        # 1b. 根据开关，加载GtoPdb的“扩展包”实体
        extra_ligands_list = []
        extra_proteins_list = []
        if data_config["use_gtopdb"]:
            print(
                "\n--- [Stage 1b] GtoPdb integration ENABLED. Loading extra entities... ---"
            )
            try:
                gtopdb_ligands_df = pd.read_csv(
                    get_path(config, "gtopdb.processed.ligands"),
                    header=None,
                    names=["CID", "SMILES"],
                )
                extra_ligands_list = gtopdb_ligands_df["SMILES"].unique().tolist()
                print(f"-> Found {len(extra_ligands_list)} unique ligands from GtoPdb.")

                gtopdb_proteins_df = pd.read_csv(
                    get_path(config, "gtopdb.processed.proteins"),
                    header=None,
                    names=["UniProt", "Sequence"],
                )
                extra_proteins_list = gtopdb_proteins_df["Sequence"].unique().tolist()
                print(
                    f"-> Found {len(extra_proteins_list)} unique proteins from GtoPdb."
                )

            except FileNotFoundError:
                print(
                    "Warning: Processed GtoPdb files not found. Continuing without GtoPdb data."
                )
        else:
            print("\n--- [Stage 1b] GtoPdb integration DISABLED. ---")
        print(
            f"DEBUG_1A: Initial unique drugs = {len(set(base_drugs_list))}, proteins = {len(set(base_proteins_list))}"
        )
        if data_config["use_gtopdb"]:
            print(
                f"DEBUG_1B: Initial unique extra ligands = {len(set(extra_ligands_list))}, extra proteins = {len(set(extra_proteins_list))}"
            )

        # In data_proc.py, replace the entire "region index"

        print("\n--- [Stage 2] Creating and saving index files... ---")

        # 1. 首先，获取所有 drug 和 ligand 的唯一SMILES集合
        unique_drug_smiles = set(base_drugs_list)
        unique_ligand_smiles = set(extra_ligands_list)

        # 2. [核心修复] 对所有分子进行统一规划和去重
        # 我们定义一个优先级规则：如果一个分子同时是drug和ligand，我们视其为drug。
        # 首先确定纯粹的ligand（只在ligand列表里出现）
        pure_ligand_smiles = unique_ligand_smiles - unique_drug_smiles

        # 所有的drug SMILES（包括那些也可能是ligand的）
        all_drug_smiles = unique_drug_smiles

        # 3. 分别对两组SMILES列表进行排序，以保证处理顺序的确定性
        sorted_unique_drugs = sorted(list(all_drug_smiles))
        sorted_unique_ligands = sorted(list(pure_ligand_smiles))
        # 2. 初始化索引字典和节点元数据列表
        drug2index = {}
        ligand2index = {}
        node_data = []
        current_id = 0

        for smile in sorted_unique_drugs:
            drug2index[smile] = current_id
            node_data.append({"node_id": current_id, "node_type": "drug"})
            current_id += 1

        for smile in sorted_unique_ligands:
            ligand2index[smile] = current_id
            node_data.append({"node_id": current_id, "node_type": "ligand"})
            current_id += 1

        # --- 统一处理所有蛋白质 (这部分逻辑不变) ---
        final_proteins_list = sorted(
            list(set(base_proteins_list + extra_proteins_list))
        )
        protein_start_index = current_id  # ID从最后一个小分子之后开始
        prot2index = {
            seq: i + protein_start_index for i, seq in enumerate(final_proteins_list)
        }
        for seq, idx in prot2index.items():
            node_data.append({"node_id": idx, "node_type": "protein"})
        print(
            f"DEBUG_2: Total indexed entities = drug({len(drug2index)}) + ligand({len(ligand2index)}) + protein({len(prot2index)}) = {len(drug2index) + len(ligand2index) + len(prot2index)}"
        )
        print(f"DEBUG_2: Total rows in node_data list = {len(node_data)}")
        # endregion index

        # --- 保存所有文件 ---
        pkl.dump(
            drug2index, open(get_path(config, checkpoint_files_dict["drug"]), "wb")
        )
        pkl.dump(
            ligand2index, open(get_path(config, checkpoint_files_dict["ligand"]), "wb")
        )
        pkl.dump(
            prot2index, open(get_path(config, checkpoint_files_dict["protein"]), "wb")
        )
        # 6. 保存节点元数据文件
        AllNode_df = pd.DataFrame(node_data)  # node_data已经包含了所有类型
        AllNode_df.to_csv(
            get_path(config, checkpoint_files_dict["nodes"]), index=False, header=True
        )
        print("-> Index and metadata files saved successfully.")

    else:
        print("\n--- [Stage 2] Loading indices and metadata from cache... ---")
        # Reuse the list defined above. No more duplication.
        drug_idx_path = get_path(config, checkpoint_files_dict["drug"])
        ligand_idx_path = get_path(config, checkpoint_files_dict["ligand"])
        prot_idx_path = get_path(config, checkpoint_files_dict["protein"])

        drug2index = pkl.load(open(drug_idx_path, "rb"))
        ligand2index = pkl.load(open(ligand_idx_path, "rb"))
        prot2index = pkl.load(open(prot_idx_path, "rb"))

    # --- 为后续步骤准备最终的、完整的实体列表 ---
    # 这些列表现在是从索引字典的键中动态生成的，而不是作为中间变量传来传去
    final_smiles_list = sorted(list(drug2index.keys())) + sorted(
        list(ligand2index.keys())
    )
    final_proteins_list = sorted(list(prot2index.keys()))
    dl2index = {**drug2index, **ligand2index}  # 统一的小分子索引字典
    print(f"DEBUG_3: Length of final_smiles_list = {len(final_smiles_list)}")
    print(f"DEBUG_3: Length of final_proteins_list = {len(final_proteins_list)}")
    print(
        f"DEBUG_3: Total nodes based on final_lists = {len(final_smiles_list) + len(final_proteins_list)}"
    )
    # endregion
    return (
        drug2index,
        ligand2index,
        prot2index,
        dl2index,
        final_smiles_list,
        final_proteins_list,
    )


def _stage_2_generate_features_and_similarities(
    config: DictConfig,
    drug2index: dict,
    ligand2index: dict,
    prot2index: dict,
    final_smiles_list,
    final_proteins_list,
    restart_flag: bool = False,
    device="cpu",
) -> tuple:
    checkpoint_files_dict = {
        "molecule_similarity_matrix": "processed.similarity_matrices.molecule",
        "protein_similarity_matrix": "processed.similarity_matrices.protein",
        "node_features": "processed.node_features",
    }
    if not check_files_exist(config, *checkpoint_files_dict.values()) or restart_flag:
        print("\n--- [Stage 3] Generating features and similarity matrices... ---")
        print("--> Initializing feature extraction models...")
        # Using your GCNLayer, but a standard GCNConv would be better.
        # .eval() is crucial: it disables dropout and other training-specific layers.
        molecule_feature_extractor = MoleculeGCN(5, 128).to(device).eval()
        protein_feature_extractor = ProteinGCN().to(device).eval()

        drug_embeddings = []
        # Use the new function signature in the loop
        for d in tqdm(
            final_smiles_list, desc="Extracting Molecule Features", leave=False
        ):
            embedding = extract_molecule_features(d, molecule_feature_extractor, device)
            drug_embeddings.append(embedding.cpu().detach().numpy())

        protein_embeddings = []
        for p in tqdm(
            final_proteins_list, desc="Extracting Protein Features", leave=False
        ):
            embedding = extract_protein_features(p, protein_feature_extractor, device)
            protein_embeddings.append(embedding.cpu().detach().numpy())

        # prot_similarity_matrix = cosine_similarity(protein_embeddings)
        dl_similarity_matrix = calculate_drug_similarity(final_smiles_list)
        # notice:用U与C一样的方式计算相似度
        prot_similarity_matrix = calculate_protein_similarity(
            final_proteins_list, config.runtime["cpus"]
        )

        pkl.dump(
            dl_similarity_matrix,
            open(
                get_path(config, checkpoint_files_dict["molecule_similarity_matrix"]),
                "wb",
            ),
        )
        pkl.dump(
            prot_similarity_matrix,
            open(
                get_path(config, checkpoint_files_dict["protein_similarity_matrix"]),
                "wb",
            ),
        )
        features_df = pd.concat(
            [pd.DataFrame(drug_embeddings), pd.DataFrame(protein_embeddings)], axis=0
        )
        features_df.to_csv(
            get_path(config, checkpoint_files_dict["node_features"]),
            index=False,
            header=False,
        )
        print(f"DEBUG_4: Shape of final features_df = {features_df.shape}")
        # --- The most critical assertion ---
        num_nodes_from_stage2 = len(drug2index) + len(ligand2index) + len(prot2index)
        assert features_df.shape[0] == num_nodes_from_stage2, (
            f"FATAL: Feature matrix length ({features_df.shape[0]}) does not match indexed node count ({num_nodes_from_stage2})!"
        )
    else:
        # 如果文件已存在，则加载它们以供后续步骤使用
        print(
            "\n--- [Stage 3] Loading features and similarity matrices from cache... ---"
        )
        dl_similarity_matrix = pkl.load(
            open(
                get_path(config, checkpoint_files_dict["molecule_similarity_matrix"]),
                "rb",
            )
        )
        prot_similarity_matrix = pkl.load(
            open(
                get_path(config, checkpoint_files_dict["protein_similarity_matrix"]),
                "rb",
            )
        )
        features_df = pd.read_csv(
            get_path(config, checkpoint_files_dict["node_features"]), header=None
        )
    return dl_similarity_matrix, prot_similarity_matrix, features_df


def _stage_3_split_data_and_build_graphs(
    config: DictConfig,
    full_df: pd.DataFrame,
    drug2index: dict,
    ligand2index: dict,
    prot2index: dict,
    dl2index: dict,
    dl_sim_matrix: np.ndarray,
    prot_sim_matrix: np.ndarray,
):
    """
    调度函数，负责Stage 3的所有工作。
    它现在只做两件事：收集正样本，然后将其交给总指挥官处理。
    """
    # 1. 收集全局的正样本对
    positive_pairs, positive_pairs_normalized_set = _collect_positive_pairs(
        config, full_df, dl2index, prot2index
    )

    # 2. 调用总指挥官，完成所有后续工作
    _process_all_folds(
        config=config,
        positive_pairs=positive_pairs,
        positive_pairs_normalized_set=positive_pairs_normalized_set,
        drug2index=drug2index,
        ligand2index=ligand2index,
        prot2index=prot2index,
        dl2index=dl2index,
        dl_sim_matrix=dl_sim_matrix,
        prot_sim_matrix=prot_sim_matrix,
    )


def _process_all_folds(
    config: DictConfig,
    positive_pairs: list,
    positive_pairs_normalized_set: set,
    drug2index: dict,
    ligand2index: dict,
    prot2index: dict,
    dl2index: dict,
    dl_sim_matrix: np.ndarray,
    prot_sim_matrix: np.ndarray,
):
    """
    一个【总指挥】函数，负责协调完成所有的分割、标签保存和图谱构建任务。
    它循环遍历每一折，并为每一折调用专属的处理函数。
    """
    print("\n--- [CONTROLLER] Starting data processing for all folds... ---")

    eval_config = config.training.evaluation
    split_mode = eval_config.mode
    seed = config.runtime.seed
    num_folds = eval_config.k_folds

    # --- 1. 准备分割迭代器 ---
    # 这部分逻辑决定了我们将如何循环（K-Fold或Single Split）
    if num_folds > 1:
        entities_to_split = None
        if split_mode in ["drug", "protein"]:
            if split_mode == "drug":
                entities_to_split = sorted(
                    list(set([p[0] for p in positive_pairs if p[0] < len(drug2index)]))
                )
            else:  # protein
                entities_to_split = sorted(list(set([p[1] for p in positive_pairs])))
            kf = KFold(n_splits=num_folds, shuffle=True, random_state=seed)
            split_iterator = kf.split(entities_to_split)
        elif split_mode == "random":
            entities_to_split = positive_pairs
            dummy_y = [
                p[1] for p in positive_pairs
            ]  # Stratify by protein to keep distribution
            skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=seed)
            split_iterator = skf.split(entities_to_split, dummy_y)
        else:
            raise ValueError(f"Unknown split_mode '{split_mode}' in config.")
    else:
        # 对于 k=1 的情况，我们创建一个只包含一个None元素的迭代器，以触发一次循环
        split_iterator = [None]
        entities_to_split = (
            positive_pairs  # 占位符，实际在_split_data_for_single_fold中处理
        )

    # --- 2. 主循环 ---
    # 遍历每一次分割，并调用下一层处理函数
    for fold_idx, split_result in enumerate(split_iterator, 1):
        print("\n" + "=" * 80)
        print(" " * 25 + f"PROCESSING FOLD {fold_idx} / {num_folds}")
        print("=" * 80)

        # --- A. 分割数据 ---
        # 调用只负责“分割”的纯函数
        train_positive_pairs, test_positive_pairs = _split_data_for_single_fold(
            split_result=split_result,
            num_folds=num_folds,
            split_mode=split_mode,
            drug2index=drug2index,
            positive_pairs=positive_pairs,
            entities_to_split=entities_to_split,
            config=config,
        )

        # --- B. 处理这一折的所有后续工作 ---
        # 调用负责“内部再分割、保存标签、构建图”的函数
        _process_single_fold(
            fold_idx=fold_idx,
            train_positive_pairs=train_positive_pairs,
            test_positive_pairs=test_positive_pairs,
            positive_pairs_normalized_set=positive_pairs_normalized_set,
            config=config,
            drug2index=drug2index,
            ligand2index=ligand2index,
            prot2index=prot2index,
            dl2index=dl2index,
            dl_sim_matrix=dl_sim_matrix,
            prot_sim_matrix=prot_sim_matrix,
        )


def _split_data_for_single_fold(
    split_result: tuple,
    num_folds: int,
    split_mode: str,
    positive_pairs: list,
    entities_to_split: list,
    config: DictConfig,
    drug2index: dict,  # 需要 drug2index 来区分药物和配体
) -> tuple[list, list]:
    """
    一个只负责【数据分割】的纯函数。

    根据传入的策略（K-Fold/Single Split, Cold/Warm Start），为单一一折
    返回训练集和测试集的正样本对列表。

    Args:
        split_result (tuple or None): KFold迭代器返回的结果。对于k=1，它为None。
        num_folds (int): 总折数 (k)。
        split_mode (str): 分割模式 ('drug', 'protein', 'random')。
        positive_pairs (list): 全局的、所有正样本对的列表。
        entities_to_split (list): 用于分割的实体列表（在冷启动时）或边列表（在热启动时）。
        config (DictConfig): 全局配置对象，用于获取seed和test_fraction。
        drug2index (dict): 药物索引，用于在冷启动时精确识别药物实体。

    Returns:
        tuple[list, list]: (train_positive_pairs, test_positive_pairs)
    """
    train_positive_pairs, test_positive_pairs = [], []
    seed = config.runtime.seed
    test_fraction = config.training.evaluation.test_fraction

    if num_folds > 1:
        # --- 路径 1: K-Fold (k > 1) 逻辑 ---
        train_indices, test_indices = split_result
        if split_mode in ["drug", "protein"]:
            # 冷启动：根据实体索引来划分边
            test_entity_ids = set([entities_to_split[i] for i in test_indices])
            entity_idx = 0 if split_mode == "drug" else 1
            train_positive_pairs = [
                p for p in positive_pairs if p[entity_idx] not in test_entity_ids
            ]
            test_positive_pairs = [
                p for p in positive_pairs if p[entity_idx] in test_entity_ids
            ]
        else:  # random (热启动)
            # 热启动：直接根据边的索引来划分
            train_positive_pairs = [entities_to_split[i] for i in train_indices]
            test_positive_pairs = [entities_to_split[i] for i in test_indices]
    else:
        # --- 路径 2: Single Split (k = 1) 逻辑 ---
        if split_mode in ["drug", "protein"]:
            # 冷启动：先分割实体，再划分边
            entity_idx = 0 if split_mode == "drug" else 1

            # [优化] 确保实体列表的提取是正确的
            if split_mode == "drug":
                # 只分割那些被定义为“药物”的实体
                entity_list = sorted(
                    list(set([p[0] for p in positive_pairs if p[0] < len(drug2index)]))
                )
            else:  # protein
                entity_list = sorted(list(set([p[1] for p in positive_pairs])))

            train_entities, test_entities = train_test_split(
                entity_list, test_size=test_fraction, random_state=seed
            )
            test_entity_ids = set(test_entities)
            train_positive_pairs = [
                p for p in positive_pairs if p[entity_idx] not in test_entity_ids
            ]
            test_positive_pairs = [
                p for p in positive_pairs if p[entity_idx] in test_entity_ids
            ]
        else:  # random (热启动)
            # 热启动：直接用train_test_split分割边
            # stratify可以保证训练集和测试集中的蛋白质分布大致相同，让评估更稳定
            labels = [p[1] for p in positive_pairs]  # 使用 protein_id 作为分层依据
            train_positive_pairs, test_positive_pairs = train_test_split(
                positive_pairs,
                test_size=test_fraction,
                random_state=seed,
                stratify=labels,
            )

    print(
        f"    -> Split complete: {len(train_positive_pairs)} TRAIN positives, {len(test_positive_pairs)} TEST positives."
    )
    return train_positive_pairs, test_positive_pairs


def _process_single_fold(
    fold_idx: int,
    train_positive_pairs: list,
    test_positive_pairs: list,
    positive_pairs_normalized_set: set,
    config: DictConfig,
    drug2index: dict,
    ligand2index: dict,
    prot2index: dict,
    dl2index: dict,
    dl_sim_matrix: np.ndarray,
    prot_sim_matrix: np.ndarray,
):
    """
    一个负责【处理单折所有后续工作】的函数。

    它接收分割好的训练/测试正样本对，然后执行：
    1. 对训练集进行内部的“背景/监督”二次分割。
    2. 为“监督集”和“测试集”生成并保存带标签的CSV文件。
    3. 使用“背景集”和相似性矩阵，构建并保存训练图谱。
    """
    print(
        f"    -> Fold {fold_idx}: Received {len(train_positive_pairs)} train and {len(test_positive_pairs)} test positives."
    )

    # --- 1. 对训练集，进行第二次“内部”分割 (Inductive Link Prediction) ---
    supervision_ratio = config.params.get("supervision_ratio", 0.2)

    # 确保即使训练集很小，也能分出至少一个监督样本
    if len(train_positive_pairs) > 1:
        train_graph_edges, train_supervision_edges = train_test_split(
            train_positive_pairs,
            test_size=supervision_ratio,
            random_state=config.runtime.seed,
        )
    else:  # 如果训练集只有一个样本，则无法分割
        train_graph_edges = train_positive_pairs
        train_supervision_edges = []  # 没有监督边，模型将只从重构损失中学习
        print("    -> WARNING: Too few training samples to create a supervision set.")

    print(
        f"    -> Internal split: {len(train_graph_edges)} edges for graph topology, "
        f"{len(train_supervision_edges)} edges for supervision."
    )

    # --- 2. 为【监督边】和【测试边】生成并保存带标签的文件 ---
    labels_template_key = "processed.link_prediction_labels_template"
    eval_config = config.training.evaluation

    # a. 训练标签文件 (基于监督边)
    train_labels_path = rt.get_path(
        config,
        labels_template_key,
        split_suffix=f"_fold{fold_idx}{eval_config.train_file_suffix}",
    )
    _generate_and_save_labeled_set(
        positive_pairs=train_supervision_edges,
        positive_pairs_normalized_set=positive_pairs_normalized_set,
        dl2index=dl2index,
        prot2index=prot2index,
        config=config,
        output_path=train_labels_path,
    )

    # b. 测试标签文件 (基于测试边)
    test_labels_path = rt.get_path(
        config,
        labels_template_key,
        split_suffix=f"_fold{fold_idx}{eval_config.test_file_suffix}",
    )
    _generate_and_save_labeled_set(
        positive_pairs=test_positive_pairs,
        positive_pairs_normalized_set=positive_pairs_normalized_set,
        dl2index=dl2index,
        prot2index=prot2index,
        config=config,
        output_path=test_labels_path,
    )
    print(f"    -> Saved labeled edges for Fold {fold_idx} successfully.")

    # --- 3. 使用【背景边】和【相似性矩阵】，构建并保存训练图谱 ---
    _build_graph_for_fold(
        fold_idx=fold_idx,
        train_graph_edges=train_graph_edges,  # <-- 使用分割出的背景边
        config=config,
        drug2index=drug2index,
        ligand2index=ligand2index,
        prot2index=prot2index,
        dl2index=dl2index,
        dl_sim_matrix=dl_sim_matrix,
        prot_sim_matrix=prot_sim_matrix,
    )


def _generate_and_save_labeled_set(
    positive_pairs: list,
    positive_pairs_normalized_set: set,
    dl2index: dict,
    prot2index: dict,
    config: DictConfig,
    output_path: Path,
):
    print(
        f"    -> Generating labeled set with {len(positive_pairs)} positive pairs for: {output_path.name}..."
    )

    negative_pairs = []
    all_molecule_ids = list(dl2index.values())
    all_protein_ids = list(prot2index.values())

    # Get the strategy from config
    sampling_strategy = config.params.negative_sampling_strategy

    with tqdm(
        total=len(positive_pairs),
        desc=f"Negative Sampling ({sampling_strategy})",
        leave=False,  # Set leave to False for cleaner logging inside a loop
    ) as pbar:
        if sampling_strategy == "popular":
            # --- Popularity-Biased Strategy ---
            mol_degrees = {}
            prot_degrees = {}
            # [MODIFICATION] Popularity is calculated based on ALL positive pairs for a stable distribution
            for (
                u,
                v,
            ) in positive_pairs_normalized_set:  # TODO: 目前使用全局popular负采样
                mol_degrees[u] = mol_degrees.get(u, 0) + 1
                prot_degrees[v] = prot_degrees.get(v, 0) + 1

            mol_weights = [mol_degrees.get(mol_id, 1) for mol_id in all_molecule_ids]
            prot_weights = [prot_degrees.get(prot_id, 1) for prot_id in all_protein_ids]

            while len(negative_pairs) < len(positive_pairs):
                dl_idx = random.choices(all_molecule_ids, weights=mol_weights, k=1)[0]
                p_idx = random.choices(all_protein_ids, weights=prot_weights, k=1)[0]
                normalized_candidate = tuple(sorted((dl_idx, p_idx)))

                # Check against the set of ALL positives to prevent generating a known interaction
                if normalized_candidate not in positive_pairs_normalized_set:
                    negative_pairs.append((dl_idx, p_idx))
                    pbar.update(1)

        elif sampling_strategy == "uniform":
            # --- Uniform Random Strategy ---
            while len(negative_pairs) < len(positive_pairs):
                dl_idx = random.choice(all_molecule_ids)
                p_idx = random.choice(all_protein_ids)
                normalized_candidate = tuple(sorted((dl_idx, p_idx)))

                if normalized_candidate not in positive_pairs_normalized_set:
                    negative_pairs.append((dl_idx, p_idx))
                    pbar.update(1)
        else:
            raise ValueError(
                f"Unknown negative_sampling_strategy: '{sampling_strategy}'"
            )

    print(f"        -> Generated {len(negative_pairs)} negative samples.")

    # --- [THIS PART IS ALSO MOVED FROM YOUR ORIGINAL CODE] ---
    # Save the unified, labeled edge file
    pos_df = pd.DataFrame(positive_pairs, columns=["source", "target"])
    pos_df["label"] = 1
    neg_df = pd.DataFrame(negative_pairs, columns=["source", "target"])
    neg_df["label"] = 0
    labeled_df = (
        pd.concat([pos_df, neg_df], ignore_index=True)
        .sample(frac=1)
        .reset_index(drop=True)
    )

    rt.ensure_path_exists(output_path)
    labeled_df.to_csv(output_path, index=False, header=True)


def _build_graph_for_fold(
    fold_idx: int,
    train_graph_edges: list,  # <-- [关键] 现在接收的是“背景知识”交互边
    config: DictConfig,
    drug2index: dict,
    ligand2index: dict,
    prot2index: dict,
    dl2index: dict,
    dl_sim_matrix: np.ndarray,
    prot_sim_matrix: np.ndarray,
):
    """
    为一个特定的Fold，构建并保存其专属的、用于GNN编码器训练的图谱文件。

    这个图谱将包含：
    1. 一部分训练集中的D-P/L-P交互，作为“背景知识”。
    2. 所有与训练集节点相关的“背景知识”相似性边。
    它【绝对不会】包含任何将用于【监督】的交互边。
    """
    print(f"\n--- [GRAPH] Building training graph for Fold {fold_idx}... ---")

    eval_config = config.training.evaluation
    split_mode = eval_config.mode
    relations_config = config.relations.flags
    graph_template_key = "processed.typed_edge_list_template"

    # --- 1. 定义当前Fold的训练节点 ---
    # 这个定义现在是基于传入的 train_graph_edges，因为它们定义了图的“骨架”
    train_node_ids = set()
    if split_mode == "drug":
        # 在药物冷启动中，训练节点 = 所有蛋白 + 所有配体 + 【只在背景知识中出现的】药物
        train_drug_ids = set([p[0] for p in train_graph_edges])
        train_node_ids = (
            set(prot2index.values())
            .union(set(ligand2index.values()))
            .union(train_drug_ids)
        )
    elif split_mode == "protein":
        # 在蛋白冷启动中，训练节点 = 所有药物 + 所有配体 + 【只在背景知识中出现的】蛋白
        train_protein_ids = set([p[1] for p in train_graph_edges])
        train_node_ids = set(dl2index.values()).union(train_protein_ids)
    else:  # random (热启动)
        # 在热启动中，所有节点都可以被认为是训练节点
        train_node_ids = set(dl2index.values()).union(set(prot2index.values()))

    print(
        f"    -> This training graph will be built upon {len(train_node_ids)} allowed nodes."
    )

    typed_edges_list = []

    # --- 2. [核心修改] 将“背景知识交互边”加入图谱 ---
    dp_added_as_background = 0
    lp_added_as_background = 0
    for u, v in train_graph_edges:
        is_drug = u < len(drug2index)
        if is_drug:
            if relations_config.get("dp_interaction", True):
                typed_edges_list.append([u, v, "drug_protein_interaction"])
                dp_added_as_background += 1
        else:  # is_ligand
            if relations_config.get("lp_interaction", True):
                typed_edges_list.append([u, v, "ligand_protein_interaction"])
                lp_added_as_background += 1

    print(
        f"    -> Added {dp_added_as_background} D-P and {lp_added_as_background} L-P interactions as background knowledge."
    )

    # --- 3. 添加相似性边 (这部分现在是增量，而不是全部) ---

    # a. 为蛋白质相似性做准备
    prot_local_id_to_type = {i: "protein" for i in range(len(prot2index))}
    prot_relation_rules = {
        ("protein", "protein"): {"edge_type": "protein_protein_similarity"}
    }

    _add_similarity_edges(
        typed_edges_list=typed_edges_list,
        sim_matrix=prot_sim_matrix,
        train_node_ids=train_node_ids,
        id_to_type_map=prot_local_id_to_type,
        relation_rules=prot_relation_rules,
        config=config,
        id_offset=len(dl2index),
    )

    # b. 为小分子相似性做准备
    mol_local_id_to_type = {i: "drug" for i, smi in enumerate(drug2index.keys())}
    mol_local_id_to_type.update(
        {i + len(drug2index): "ligand" for i, smi in enumerate(ligand2index.keys())}
    )
    mol_relation_rules = {
        ("drug", "drug"): {"edge_type": "drug_drug_similarity"},
        ("ligand", "ligand"): {"edge_type": "ligand_ligand_similarity"},
        ("drug", "ligand"): {
            "edge_type": "drug_ligand_similarity",
            "source_priority": "drug",
        },
    }

    _add_similarity_edges(
        typed_edges_list=typed_edges_list,
        sim_matrix=dl_sim_matrix,
        train_node_ids=train_node_ids,
        id_to_type_map=mol_local_id_to_type,
        relation_rules=mol_relation_rules,
        config=config,
        id_offset=0,
    )

    # --- 4. 保存最终的图文件 ---
    graph_output_path = rt.get_path(
        config, graph_template_key, split_suffix=f"_fold{fold_idx}"
    )
    print(
        f"\n--> Saving final graph structure for Fold {fold_idx} to: {graph_output_path}"
    )

    typed_edges_df = pd.DataFrame(
        typed_edges_list, columns=["source", "target", "edge_type"]
    )
    typed_edges_df.to_csv(graph_output_path, index=False, header=True)

    print(f"-> Total edges in the final training graph: {len(typed_edges_df)}")


def _add_similarity_edges(
    typed_edges_list: list,
    sim_matrix: np.ndarray,
    train_node_ids: set,
    id_to_type_map: dict,
    relation_rules: dict,
    config: DictConfig,
    id_offset: int = 0,  # [核心新增] ID偏移量，默认为0
):
    """
    一个【最终版】的、通用的、可扩展的辅助函数，用于从一个相似度矩阵中添加多种类型的边。
    它现在可以动态地为每种关系查找专属的阈值。
    """
    print(f"--> Processing similarity matrix of shape {sim_matrix.shape}...")

    counts = {rule["edge_type"]: 0 for rule in relation_rules.values()}

    # --- [核心逻辑] 现在我们遍历规则，而不是遍历矩阵中的所有边 ---
    # 这使得我们可以为每个规则（即每种边类型）应用不同的阈值
    for rule_key, rule in relation_rules.items():
        edge_type = rule["edge_type"]
        relation_flag = edge_type.replace("_similarity", "")

        # 1. 检查配置开关
        if not config.relations.flags.get(relation_flag, True):
            continue

        # 2. [核心修改] 动态构造阈值参数的键名，并从配置中读取
        threshold_key = (
            f"{rule_key[0]}_{rule_key[1]}"  # e.g., 'drug_drug', 'drug_ligand'
        )
        try:
            threshold = config.params.similarity_thresholds[threshold_key]
        except Exception:
            print(
                f"    - WARNING: Threshold key 'params.similarity_thresholds.{threshold_key}' not found. Skipping '{edge_type}'."
            )
            continue

        print(f"    -> Processing '{edge_type}' with threshold > {threshold}...")

        # 3. 寻找超过阈值的边
        rows, cols = np.where(sim_matrix > threshold)

        # 4. 添加边
        for i, j in zip(rows, cols):
            if i >= j:
                continue
            global_id_i = i + id_offset
            global_id_j = j + id_offset

            # [关键] 过滤步骤现在使用全局ID
            if global_id_i not in train_node_ids or global_id_j not in train_node_ids:
                continue

            # 查询类型（现在使用局部索引）
            type1 = id_to_type_map.get(i)
            type2 = id_to_type_map.get(j)

            if not type1 or not type2:
                continue

            if tuple(sorted((type1, type2))) != rule_key:
                continue  # 这条边不属于当前规则处理的范畴

            # 根据规则决定边的方向
            source, target = (
                (global_id_i, global_id_j)
                if type1 == rule.get("source_priority", type1)
                else (global_id_j, global_id_i)
            )

            typed_edges_list.append([source, target, edge_type])
            counts[edge_type] += 1

    for edge_type, count in counts.items():
        if count > 0:
            print(f"    - Added {count} '{edge_type}' edges.")


def _collect_positive_pairs(
    config: DictConfig, full_df: pd.DataFrame, dl2index: dict, prot2index: dict
) -> list:
    data_config = config["data"]
    eval_config = config.training.evaluation
    split_mode = eval_config.mode
    print(f"--> Preparing and splitting positive pairs with mode: '{split_mode}'...")
    print(
        "--> Labeled train/test files not found or restart forced. Generating new splits..."
    )
    positive_pairs_normalized_set = set()

    # Filter the DataFrame for positive labels first
    positive_interactions_df = full_df[full_df["Y"] == 1].copy()

    # Pre-compute a mapping from raw to canonical SMILES to avoid re-computation
    raw_smiles_in_pos = positive_interactions_df["SMILES"].dropna().unique()
    canonical_map = {s: canonicalize_smiles(s) for s in raw_smiles_in_pos}
    positive_interactions_df["canonical_smiles"] = positive_interactions_df[
        "SMILES"
    ].map(canonical_map)

    for _, row in tqdm(
        positive_interactions_df.iterrows(),
        total=len(positive_interactions_df),
        desc="Processing Interaction Pairs",
    ):
        canonical_s, protein = row["canonical_smiles"], row["Protein"]
        if pd.notna(canonical_s) and canonical_s in dl2index and protein in prot2index:
            d_idx, p_idx = dl2index[canonical_s], prot2index[protein]
            normalized_pair = tuple(sorted((d_idx, p_idx)))
            positive_pairs_normalized_set.add(normalized_pair)

    # Scan interactions from GtoPdb if enabled
    if data_config["use_gtopdb"]:
        print("--> Scanning positive interactions from GtoPdb...")
        gtopdb_edges_df = pd.read_csv(
            rt.get_path(config, "gtopdb.processed.interactions"),
            header=None,
            names=["Sequence", "SMILES", "Affinity"],
        )
        for _, row in tqdm(
            gtopdb_edges_df.iterrows(),
            total=len(gtopdb_edges_df),
            desc="GtoPdb Pairs",
        ):
            smiles, sequence = row["SMILES"], row["Sequence"]
            if smiles in dl2index and sequence in prot2index:
                l_idx, p_idx = dl2index[smiles], prot2index[sequence]
                normalized_pair = tuple(sorted((l_idx, p_idx)))
                positive_pairs_normalized_set.add(normalized_pair)

    positive_pairs = list(positive_pairs_normalized_set)
    print(
        f"-> Found {len(positive_pairs)} unique, normalized positive pairs after cleaning."
    )
    return positive_pairs, positive_pairs_normalized_set
# src/data_utils/loaders.py

import pandas as pd
import torch
from torch_geometric.data import HeteroData
from omegaconf import DictConfig
import torch_geometric.transforms as T

# [优化] 将 research_template 的导入也放在这里
# 这样，所有与路径管理相关的依赖都集中在了这个文件中
import research_template as rt


def create_global_to_local_maps(config: DictConfig) -> dict:
    """从nodes.csv创建全局到局部的ID映射字典。"""
    nodes_df = pd.read_csv(rt.get_path(config, "processed.nodes_metadata"))
    maps = {}
    node_type_groups = nodes_df.groupby("node_type")
    for node_type in sorted(node_type_groups.groups.keys()):
        group = node_type_groups.get_group(node_type)

        # group['node_id'].values 保证了我们是按照全局ID的顺序来创建局部ID
        maps[node_type] = {
            global_id: local_id
            for local_id, global_id in enumerate(group["node_id"].values)
        }

    return maps


def convert_df_to_local_tensors(
    df: pd.DataFrame,
    global_to_local_maps: dict,
    src_node_type: str,
    dst_node_type: str,
    device: torch.device,
) -> tuple[torch.Tensor, torch.Tensor]:
    """
    一个辅助函数，将包含全局ID的DataFrame转换为包含局部ID的PyTorch张量。
    """
    # 1. 使用映射字典进行ID转换
    src_local = torch.tensor(
        [global_to_local_maps[src_node_type][gid] for gid in df["source"]],
        dtype=torch.long,
    )
    dst_local = torch.tensor(
        [global_to_local_maps[dst_node_type][gid] for gid in df["target"]],
        dtype=torch.long,
    )

    # 2. 准备边索引张量和标签张量
    edge_label_index = torch.stack([src_local, dst_local]).to(device)
    edge_label = torch.from_numpy(df["label"].values).to(device)
    return edge_label_index, edge_label


def load_graph_data_for_fold(
    config: DictConfig,
    fold_idx: int,
    global_to_local_maps: dict,  # <-- [关键] 接收“权威指南”
) -> HeteroData:
    """
    [最终正确版] 为指定的Fold加载图结构，并使用【外部传入】的ID映射来构建一个
    包含【局部ID】的HeteroData对象。

    这个版本能够正确处理所有节点类型（包括ligand）和所有边类型。
    """
    print(
        f"--- [Loader] Loading and building graph with LOCAL IDs for Fold {fold_idx}... ---"
    )

    # 1. 加载节点数据
    nodes_df = pd.read_csv(rt.get_path(config, "processed.nodes_metadata"))
    features_tensor = torch.from_numpy(
        pd.read_csv(rt.get_path(config, "processed.node_features"), header=None).values
    ).float()

    data = HeteroData()

    # 2. 填充节点特征
    # [优化] 使用我们传入的映射字典的键，来保证顺序一致性
    for node_type in global_to_local_maps.keys():
        # 从 nodes_df 中筛选出当前类型的所有节点
        mask = nodes_df["node_type"] == node_type
        # 按照全局ID，从总特征张量中提取出当前类型的特征
        data[node_type].x = features_tensor[nodes_df[mask]["node_id"].values]

    # 3. 加载边数据，并使用【传入的】映射转换为局部ID
    edges_path = rt.get_path(
        config, "processed.typed_edge_list_template", split_suffix=f"_fold{fold_idx}"
    )
    edges_df = pd.read_csv(edges_path)

    # [优化] 这个临时的id_to_type_str现在变得更可靠，因为它也基于nodes_df
    id_to_type_str = pd.Series(
        nodes_df.node_type.values, index=nodes_df.node_id
    ).to_dict()

    for edge_type_str, group in edges_df.groupby("edge_type"):
        sources_global = group["source"].values
        targets_global = group["target"].values

        source_type = id_to_type_str[sources_global[0]]
        target_type = id_to_type_str[targets_global[0]]
        edge_type_tuple = (source_type, edge_type_str, target_type)

        # [关键] 使用由 train.py 传入的、唯一的 global_to_local_maps
        source_local_ids = [
            global_to_local_maps[source_type][gid] for gid in sources_global
        ]
        target_local_ids = [
            global_to_local_maps[target_type][gid] for gid in targets_global
        ]

        data[edge_type_tuple].edge_index = torch.tensor(
            [source_local_ids, target_local_ids], dtype=torch.long
        )
    data = T.ToUndirected(merge=True)(data)
    print(f"--- HeteroData object for Fold {fold_idx} constructed successfully. ---")
    return data


def load_labels_for_fold(
    config: DictConfig,
    fold_idx: int,
) -> tuple[pd.DataFrame, pd.DataFrame]:
    """
    为指定的Fold加载带标签的训练和测试边集。
    """
    print(f"--- [Loader] Loading labeled edges for Fold {fold_idx}... ---")

    lp_labels_key = "processed.link_prediction_labels_template"
    train_suffix = config.training.evaluation.train_file_suffix
    test_suffix = config.training.evaluation.test_file_suffix

    train_path = rt.get_path(
        config, lp_labels_key, split_suffix=f"_fold{fold_idx}{train_suffix}"
    )
    test_path = rt.get_path(
        config, lp_labels_key, split_suffix=f"_fold{fold_idx}{test_suffix}"
    )

    train_df = pd.read_csv(train_path)
    test_df = pd.read_csv(test_path)

    return train_df, test_df
import torch
import torch.nn.functional as F
import pandas as pd
import numpy as np
from scipy.sparse import coo_matrix
from research_template import sparse_mx_to_torch_sparse_tensor
from .ndls_homo_utils import (
    augmented_random_walk_normalization,
    aver,
    aver_smooth_vectorized,
    diffuse_features_on_homo_graph,
    localize_optimal_hops,
)
import tqdm as tqdm
import torch.nn as nn


class DNNRefiner(nn.Module):
    """
    A simple two-layer fully-connected network to refine node features.
    This mimics the hidden DNN layer from the original implementation.
    """

    def __init__(
        self, in_channels: int, hidden_channels: int, out_channels: int, dropout: float
    ):
        super().__init__()
        self.fcn1 = nn.Linear(in_channels, hidden_channels)
        self.fcn2 = nn.Linear(hidden_channels, out_channels)
        self.dropout = dropout

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Note: The original code applies dropout BEFORE the first layer, which is unusual.
        # We will replicate this behavior for perfect reproduction.
        x = F.dropout(x, self.dropout, training=self.training)
        x = self.fcn1(x)
        x = F.dropout(x, self.dropout, training=self.training)
        x = self.fcn2(x)
        # The original code returns log_softmax and the raw embeddings. We only need the embeddings.
        return x


class NDLS_Homo_Encoder:
    """
    Computes node embeddings on a homogeneous graph using the NDLS algorithm.
    This class encapsulates the two-stage (unsupervised) embedding generation process.
    """

    def __init__(self, config: dict, device: torch.device):
        """
        Initializes the encoder with configuration and device.
        """
        self.config = config
        self.params = config.encoder
        self.device = device
        self.embeddings = None
        self.refiner = None
        if self.params.refiner.enabled:
            print("--> DNN Refiner is ENABLED.")
            # We don't know the input dimension yet, so we'll instantiate it in fit()
        else:
            print("--> DNN Refiner is DISABLED.")

    def fit(self, adj: coo_matrix, features: torch.Tensor):
        """
        The main method to compute and store the node embeddings.
        This contains the core logic from your old main.py.
        """
        print("--- [Encoder] Fitting NDLS_Homo_Encoder... ---")
        k1 = self.params["k1"]
        epsilon1 = self.params["epsilon1"]

        print("--> Stage 1: Pre-computing feature diffusions and stability anchor...")

        # Calculate the stationary distribution (norm_a_inf) for the stability anchor
        node_sum = adj.shape[0]
        edge_sum = adj.sum() / 2
        # Add 1 to row_sum for stability, especially for isolated nodes
        row_sum = np.array(adj.sum(1)).flatten() + 1
        norm_a_inf = (
            torch.tensor(row_sum / (2 * edge_sum + node_sum), dtype=torch.float32)
            .view(1, -1)
            .to(self.device)
        )

        # Normalize the adjacency matrix using the NDLS-specific method
        adj_norm = sparse_mx_to_torch_sparse_tensor(
            augmented_random_walk_normalization(adj)
        ).to(self.device)

        # L1-normalize features and move to the target device
        features_norm = F.normalize(features, p=1).to(self.device)

        # Pre-compute K1 hops of diffused features
        feature_list = diffuse_features_on_homo_graph(features_norm, adj_norm, k1)

        # Compute the global stability anchor feature
        norm_fea_inf = torch.mm(norm_a_inf, features_norm)

        # ===================================================================
        # 2. Localization Stage
        # ===================================================================
        print("--> Stage 2: Localizing optimal hop for each node...")

        hops = localize_optimal_hops(feature_list, norm_fea_inf, epsilon1)

        # ===================================================================
        # 3. Smoothing Stage
        # ===================================================================
        print("--> Stage 3: Performing final smoothing...")
        smoothing_mode = self.params.get("smoothing_mode", "lookup")

        if smoothing_mode == "iterative":
            print("    -> Using 'iterative' smoothing mode.")
            alpha = self.params.get("smoothing_alpha", 0.15)
            # Call our new, efficient, vectorized function
            smoothed_features = aver_smooth_vectorized(
                hops.cpu(), [f.cpu() for f in feature_list], alpha
            )
        else:  # Default to our original 'lookup' method
            print("    -> Using 'lookup' smoothing mode.")
            smoothed_features = aver(hops.cpu(), adj, [f.cpu() for f in feature_list])

        self.embeddings = pd.DataFrame(smoothed_features.numpy())
        print(f"--> NDLS embeddings generated with shape: {self.embeddings.shape}")

        if self.params.refiner.enabled:
            print("--> Stage 4: Applying DNN Feature Refiner...")

            # 1. Instantiate the refiner now that we know the input dimension
            refiner_params = self.params.refiner
            self.refiner = DNNRefiner(
                in_channels=smoothed_features.shape[1],
                hidden_channels=refiner_params.hidden_channels,
                out_channels=refiner_params.out_channels,
                dropout=refiner_params.dropout,
            ).to(self.device)

            # 2. Put the refiner in evaluation mode (important for dropout)
            self.refiner.eval()

            # 3. Perform a forward pass to get the refined embeddings
            # The operation is unsupervised, so no backpropagation is needed.
            with torch.no_grad():
                final_embeddings_tensor = self.refiner(
                    smoothed_features.to(self.device)
                )

            # Move embeddings back to CPU for GBDT
            final_embeddings_tensor = final_embeddings_tensor.cpu()
        else:
            # If refiner is disabled, just use the smoothed features directly
            final_embeddings_tensor = smoothed_features

        self.embeddings = pd.DataFrame(final_embeddings_tensor.numpy())
        print(f"--> NDLS embeddings generated with shape: {self.embeddings.shape}")

        return self

    def get_embeddings(self) -> pd.DataFrame:
        """
        Returns the computed embeddings as a pandas DataFrame.

        Raises:
            RuntimeError: If embeddings have not been computed yet via .fit().

        Returns:
            pd.DataFrame: A DataFrame where the index corresponds to the global node ID
                          and columns are the feature dimensions.
        """
        if self.embeddings is None:
            raise RuntimeError(
                "Embeddings have not been computed yet. Please call .fit() first."
            )
        return self.embeddings.values
import numpy as np
import scipy.sparse as sp
import torch
from scipy.sparse import coo_matrix
from tqdm import tqdm

# region homo_ndls


def convert_hetero_to_homo(hetero_data) -> tuple:
    """
    Converts a HeteroData object (representing an undirected graph)
    to a symmetric, homogeneous SciPy adjacency matrix.
    """
    print("--> Converting HeteroData to Homogeneous Graph for NDLS...")
    from scipy.sparse import coo_matrix

    num_nodes = hetero_data.num_nodes
    features = torch.cat(
        [hetero_data[node_type].x for node_type in hetero_data.node_types], dim=0
    )

    node_offsets = {}
    current_offset = 0
    for node_type in hetero_data.node_types:
        node_offsets[node_type] = current_offset
        current_offset += hetero_data[node_type].num_nodes

    # --- [SIMPLIFIED & CORRECTED LOGIC] ---

    # 1. Collect all unique undirected edges from HeteroData in global indices
    all_edges = []
    for edge_type in hetero_data.edge_types:
        source_type, _, target_type = edge_type
        edge_index = hetero_data[edge_type].edge_index

        offset_edge_index = torch.stack(
            [
                edge_index[0] + node_offsets[source_type],
                edge_index[1] + node_offsets[target_type],
            ]
        )
        all_edges.append(offset_edge_index)

    # Concatenate all edges defined in the HeteroData object
    edge_index_global = torch.cat(all_edges, dim=1)

    # 2. Create the symmetric (undirected) adjacency matrix
    # We add both (u, v) and (v, u) to the edge list for the COO matrix
    full_edge_index = torch.cat(
        [edge_index_global, torch.stack([edge_index_global[1], edge_index_global[0]])],
        dim=1,
    )

    # Note: We do NOT use .unique() here anymore, because data_proc.py
    # has already guaranteed the uniqueness of undirected edges.
    # If we still want to be safe, unique can be used on the final full_edge_index.
    full_edge_index = full_edge_index.unique(dim=1)

    adj = coo_matrix(
        (
            np.ones(full_edge_index.shape[1]),
            (full_edge_index[0].numpy(), full_edge_index[1].numpy()),
        ),
        shape=(num_nodes, num_nodes),
    )

    # The number of undirected edges is now correctly adj.nnz / 2
    print(
        f"--> Homogeneous graph constructed: {adj.shape[0]} nodes, {adj.nnz // 2} edges."
    )
    return adj, features, node_offsets


# endregion


def augmented_random_walk_normalization(adj: sp.spmatrix) -> sp.spmatrix:
    """
    Performs augmented random walk normalization (D_hat^-1 * A_hat) on an adjacency matrix.
    This is a specific normalization used in algorithms like NDLS.

    A_hat = A + I (Adjacency matrix with self-loops)
    D_hat is the diagonal degree matrix of A_hat.

    Args:
        adj (sp.spmatrix): The input adjacency matrix (A).

    Returns:
        sp.spmatrix: The normalized sparse matrix.
    """
    # Add self-loops
    adj_hat = adj + sp.eye(adj.shape[0])

    # Calculate the degree matrix D_hat
    row_sum = np.array(adj_hat.sum(1))

    # [CRITICAL FIX] Handle nodes with degree 0
    d_inv = np.power(row_sum, -1.0).flatten()
    d_inv[np.isinf(d_inv)] = 0.0  # Set inf to 0 for isolated nodes
    d_mat_inv = sp.diags(d_inv)

    # Calculate D_hat^-1 * A_hat
    normalized_adj = d_mat_inv.dot(adj_hat)

    return normalized_adj.tocoo()


def aver(
    hops: torch.Tensor, adj: coo_matrix, feature_list: list[torch.Tensor]
) -> torch.Tensor:
    """
    Assembles the final node embeddings based on the personalized hop counts (NDLS smoothing).

    For each node `i`, it selects the feature vector from `feature_list[h_i]`,
    where `h_i` is the optimal hop count for node `i` stored in the `hops` tensor.

    Args:
        hops (torch.Tensor): A 1D tensor of shape [num_nodes], where each element
                             is an integer representing the optimal hop for that node.
        adj (coo_matrix): The original adjacency matrix (its shape is used to get num_nodes).
                          This argument is kept for compatibility with the original NDLS code,
                          though it's not strictly needed if num_nodes is passed directly.
        feature_list (list[torch.Tensor]): A list where feature_list[k] is the feature
                                           matrix aggregated over a k-hop neighborhood.

    Returns:
        torch.Tensor: The final smoothed node feature matrix of shape [num_nodes, feature_dim].
    """
    num_nodes = adj.shape[0]
    feature_dim = feature_list[0].shape[1]

    # 1. 创建一个空的张量来存放最终的特征
    # Ensure it's on the same device as the feature_list tensors
    output_features = torch.zeros(num_nodes, feature_dim, device=feature_list[0].device)

    # 2. 将hops张量转换为整数类型，以便用作索引
    # The hops tensor from NDLS might be float initially.
    hops = hops.long()

    # 3. [核心逻辑] 利用PyTorch的高级索引，并行地执行“查表与组装”
    # 我们遍历所有可能的hop值（从0到最大hop）
    for h in torch.unique(hops):
        # a. 找到所有最佳hop值等于当前 h 的节点的索引
        # `torch.where` returns a tuple, we need the first element
        node_indices_for_hop_h = torch.where(hops == h)[0]

        # b. 从对应的特征矩阵 feature_list[h] 中，
        #    根据这些索引，一次性地“切片”出所有这些节点的特征向量。
        features_for_hop_h = feature_list[h][node_indices_for_hop_h]

        # c. 将切片出的特征向量，填充到输出矩阵的正确位置。
        output_features[node_indices_for_hop_h] = features_for_hop_h

    return output_features


# In src/encoders/ndls_utils.py


def aver_smooth_vectorized(
    hops: torch.Tensor, feature_list: list[torch.Tensor], alpha: float = 0.15
) -> torch.Tensor:
    """
    An efficient, vectorized implementation of the iterative smoothing with teleportation.

    Args:
        hops (torch.Tensor): A 1D tensor of optimal hop for each node.
        feature_list (list[torch.Tensor]): List of diffused features at each hop.
        alpha (float): The teleport probability to the initial features.

    Returns:
        torch.Tensor: The final smoothed node feature matrix.
    """
    num_nodes, feature_dim = feature_list[0].shape
    device = feature_list[0].device

    # --- 1. Pre-calculate the cumulative sum of features ---
    # F_sum[k] will store the sum of features from hop 0 to k.
    feature_stack = torch.stack(feature_list, dim=0)  # Shape: [k, num_nodes, dim]
    F_sum = torch.cumsum(feature_stack, dim=0)  # Shape: [k, num_nodes, dim]

    # --- 2. Initialize the output tensor ---
    output_features = torch.zeros(num_nodes, feature_dim, device=device)
    hops = hops.long()

    # --- 3. Vectorized processing for each unique hop value ---
    for h in torch.unique(hops):
        if h == 0:
            # Nodes with hop 0 just use their original features
            node_indices = torch.where(hops == h)[0]
            output_features[node_indices] = feature_list[0][node_indices]
        else:
            node_indices = torch.where(hops == h)[0]

            # Get the cumulative sum of features up to hop h-1 for these nodes
            # Shape: [num_selected_nodes, dim]
            sum_f_j = F_sum[h - 1, node_indices, :]

            # Get the initial features (at hop 0) for these nodes
            f_0 = feature_list[0][node_indices]

            # Apply the formula in a single vectorized operation
            # final_feature = ((1-alpha) * sum(F_j) + h * alpha * F_0) / h
            final_features_h = ((1 - alpha) * sum_f_j + h * alpha * f_0) / h

            output_features[node_indices] = final_features_h

    return output_features


def diffuse_features_on_homo_graph(
    features: torch.Tensor, adj_norm: torch.Tensor, k: int
) -> list[torch.Tensor]:
    """
    Performs K-hop feature diffusion on a homogeneous graph.

    Args:
        features (torch.Tensor): Initial node features.
        adj_norm (torch.Tensor): Normalized adjacency matrix for propagation.
        k (int): The maximum number of diffusion steps.

    Returns:
        list[torch.Tensor]: A list where list[i] contains features after i hops.
    """
    print(f"--> Pre-computing {k}-hop feature diffusions...")
    feature_list = [features]
    for _ in tqdm(range(1, k), desc="Feature Diffusion", leave=False):
        feature_list.append(torch.spmm(adj_norm, feature_list[-1]))
    return feature_list


# Location: src/encoders/ndls_utils.py or src/encoders/ops.py


def localize_optimal_hops(
    feature_list: list[torch.Tensor], anchor_features: torch.Tensor, epsilon: float
) -> torch.Tensor:
    """
    Finds the optimal hop count for each node based on its feature distance to an anchor.

    Args:
        feature_list (list[torch.Tensor]): List of diffused features at each hop.
        anchor_features (torch.Tensor): A global anchor feature vector for comparison.
        epsilon (float): The distance threshold.

    Returns:
        torch.Tensor: A 1D tensor containing the optimal hop for each node.
    """
    print(f"--> Localizing optimal hops with epsilon={epsilon}...")
    num_nodes = feature_list[0].shape[0]
    max_hops = len(feature_list)
    device = feature_list[0].device

    hops = torch.zeros(num_nodes, dtype=torch.long, device=device)
    mask_before = torch.zeros(num_nodes, dtype=torch.bool, device=device)

    for i in tqdm(range(max_hops), desc="Hop Localization", leave=False):
        dist = torch.norm(feature_list[i] - anchor_features, p=2, dim=1)
        mask = (dist < epsilon) & ~mask_before
        hops[mask] = i
        mask_before |= mask

    hops[~mask_before] = max_hops - 1
    return hops
# src/encoders/rgcn_hetero_encoder.py

import torch
from torch_geometric.nn import HeteroConv, RGCNConv


class RGCNHeteroEncoder(torch.nn.Module):
    """
    A heterogeneous Graph Convolutional Network Encoder using the RGCNConv layer.

    This encoder is designed to work with PyG's HeteroData objects. It uses the
    powerful HeteroConv wrapper to apply RGCN convolutions to different edge
    types in the graph.
    """

    def __init__(
        self,
        in_channels_dict: dict,
        hidden_channels: int,
        out_channels: int,
        num_layers: int,
        dropout: float,
        hetero_metadata: tuple,
    ):
        """
        Initializes the RGCN Hetero Encoder.

        Args:
            in_channels_dict (dict): A dictionary mapping each node type (str) to its
                                     input feature dimension (int).
                                     Example: {'drug': 128, 'protein': 128, 'ligand': 128}
            hidden_channels (int): The number of channels in the hidden layers.
            out_channels (int): The number of channels in the output embeddings.
            num_layers (int): The total number of GNN layers.
            dropout (float): The dropout probability.
            hetero_metadata (tuple): The metadata of the HeteroData object, typically
                                     obtained from `data.metadata()`. It's a tuple
                                     of (node_types, edge_types).
        """
        super().__init__()

        self.node_types, self.edge_types = hetero_metadata

        # --- 1. Input Projections (Linear Layers) ---
        # It's a best practice to project features of different node types to the
        # same hidden dimension before the first message passing layer.
        self.proj = torch.nn.ModuleDict()
        for node_type, in_channels in in_channels_dict.items():
            self.proj[node_type] = torch.nn.Linear(in_channels, hidden_channels)

        # --- 2. Message Passing Layers (HeteroConv) ---
        self.convs = torch.nn.ModuleList()
        for _ in range(num_layers):
            # For each layer, we create a HeteroConv wrapper.
            # This wrapper holds a specific convolution layer for each edge type.
            conv = HeteroConv(
                {
                    # For each edge type, we specify an RGCNConv layer.
                    # The '-1' for in_channels is a PyG convention that allows it to
                    # automatically infer the correct input dimensions for source and
                    # target nodes.
                    edge_type: RGCNConv(
                        in_channels=-1,
                        out_channels=hidden_channels,
                        num_relations=len(self.edge_types),
                    )
                    for edge_type in self.edge_types
                },
                aggr="sum",
            )  # 'aggr' specifies how to aggregate results for nodes that
            # receive messages from different edge types. 'sum' is common.
            self.convs.append(conv)

        # --- 3. Final Projection (Optional, but good practice) ---
        # This layer projects the final hidden embeddings to the desired output dimension.
        self.lin = torch.nn.ModuleDict()
        for node_type in self.node_types:
            self.lin[node_type] = torch.nn.Linear(hidden_channels, out_channels)

        self.dropout = torch.nn.Dropout(p=dropout)

    def forward(self, x_dict: dict, edge_index_dict: dict) -> dict:
        """
        The forward pass of the encoder.

        Args:
            x_dict (dict): The dictionary of node features.
            edge_index_dict (dict): The dictionary of edge indices.

        Returns:
            dict: A dictionary of the final node embeddings for each node type.
        """
        # 1. Apply initial projection and activation
        x_dict = {
            node_type: self.proj[node_type](x).relu() for node_type, x in x_dict.items()
        }

        # 2. Propagate through the HeteroConv layers
        for conv in self.convs:
            # The HeteroConv wrapper takes the node feature and edge index dictionaries
            # and internally handles the message passing for all edge types.
            x_dict_update = conv(x_dict, edge_index_dict)

            # Apply activation and dropout after each layer
            x_dict = {
                node_type: self.dropout(x.relu())
                for node_type, x in x_dict_update.items()
            }

        # 3. Apply final linear projection
        x_dict = {node_type: self.lin[node_type](x) for node_type, x in x_dict.items()}

        return x_dict
from rdkit import Chem
import torch
from torch_geometric.data import Data
import torch.nn.functional as F
from torch_geometric.nn import GCNConv


# region d/l feature
# --- FIXED VERSION of canonicalize_smiles ---
def canonicalize_smiles(smiles):
    mol = Chem.MolFromSmiles(smiles)
    if mol is not None:
        return Chem.MolToSmiles(mol, canonical=True)
    else:
        print(f"Warning: Invalid SMILES string found and will be ignored: {smiles}")
        return None  # Return None for invalid SMILES


def smiles_to_graph(smiles):
    mol = Chem.MolFromSmiles(smiles)
    if mol is None:
        raise ValueError("Invalid SMILE string")

    # 提取原子特征
    atom_features = []
    for atom in mol.GetAtoms():
        atom_features.append(
            [
                atom.GetAtomicNum(),  # 原子序数
                atom.GetDegree(),  # 成键数
                atom.GetTotalNumHs(),  # 附着氢原子数量
                atom.GetFormalCharge(),  # 价态
                int(atom.GetIsAromatic()),  # 是否为芳环
            ]
        )

    atom_features = torch.tensor(atom_features, dtype=torch.float)

    if mol.GetNumBonds() > 0:
        edges = []
        for bond in mol.GetBonds():
            edges.append((bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()))
            edges.append((bond.GetEndAtomIdx(), bond.GetBeginAtomIdx()))  # Undirected

        # This creates a [num_bonds * 2, 2] tensor, then transposes to [2, num_bonds * 2]
        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()
    else:
        # If there are no bonds, create an empty edge_index of the correct shape [2, 0]
        edge_index = torch.empty((2, 0), dtype=torch.long)

    # 转换为PyG数据格式
    x = atom_features
    data = Data(x=x, edge_index=edge_index)
    return data


class MoleculeGCN(torch.nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.conv1 = GCNConv(in_channels, 64)
        self.conv2 = GCNConv(64, out_channels)

    def forward(self, x, edge_index):
        x = F.relu(self.conv1(x, edge_index))
        x = self.conv2(x, edge_index)
        return x


# 特征提取
def extract_molecule_features(smiles, model, device):
    graph_data = smiles_to_graph(smiles)
    # 在PyG的GCNConv中, x的形状需要是 [num_nodes, in_channels]
    x, edge_index = graph_data.x.to(device), graph_data.edge_index.to(device)
    node_embeddings = model(x, edge_index)  # 直接调用模型
    molecule_embedding = node_embeddings.mean(dim=0)
    return molecule_embedding


# endregion

# region p feature
# 氨基酸编码
aa_index = {
    "A": 0,
    "R": 1,
    "N": 2,
    "D": 3,
    "C": 4,
    "E": 5,
    "Q": 6,
    "G": 7,
    "H": 8,
    "I": 9,
    "L": 10,
    "K": 11,
    "M": 12,
    "F": 13,
    "P": 14,
    "S": 15,
    "T": 16,
    "W": 17,
    "Y": 18,
    "V": 19,
    "X": 20,
    "U": 21,
}


# 将氨基酸序列转换为图数据
def aa_sequence_to_graph(sequence):
    # 提取氨基酸特征
    node_features = torch.tensor(
        [aa_index[aa] for aa in sequence], dtype=torch.float
    ).unsqueeze(1)

    # 构建邻接矩阵
    edges = []
    for i in range(len(sequence) - 1):
        edges.append((i, i + 1))
        edges.append((i + 1, i))  # 无向图

    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()

    # 转换为PyG数据格式
    x = node_features
    data = Data(x=x, edge_index=edge_index)
    return data


# 定义GCN模型
class ProteinGCN(
    torch.nn.Module,
):
    def __init__(self):
        super(ProteinGCN, self).__init__()
        self.conv1 = GCNConv(1, 64)
        self.conv2 = GCNConv(64, 128)  # 20为氨基酸种类数

    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        x = F.relu(self.conv1(x, edge_index))
        x = F.dropout(x, training=self.training)
        x = self.conv2(x, edge_index)
        return F.log_softmax(x, dim=1)


# 特征提取
def extract_protein_features(sequence: str, model, device):
    graph_data = aa_sequence_to_graph(sequence)
    node_embeddings = model(graph_data.to(device))
    # 使用平均池化作为读出函数
    protein_embedding = node_embeddings.mean(dim=0)
    return protein_embedding


# endregion
from Bio import Align
from Bio.Align import substitution_matrices
from joblib import Parallel, delayed
from rdkit.Chem import AllChem
from rdkit.Chem.rdFingerprintGenerator import GetMorganGenerator
import numpy as np
from tqdm import tqdm
from rdkit import Chem


def calculate_drug_similarity(drug_list):
    # 将SMILES转换为RDKit分子对象
    molecules = [Chem.MolFromSmiles(smiles) for smiles in drug_list]
    fpgen = GetMorganGenerator(radius=2, fpSize=2048)
    fingerprints = [fpgen.GetFingerprint(mol) for mol in molecules]

    num_molecules = len(molecules)
    similarity_matrix = np.zeros((num_molecules, num_molecules))

    # 2. 将外层循环用 tqdm 包裹起来
    # desc 参数为进度条提供了一个清晰的描述
    for i in tqdm(
        range(num_molecules), desc="Calculating Drug Similarities", leave=False
    ):
        # 由于矩阵是对称的，我们可以只计算上三角部分来优化性能
        for j in range(i, num_molecules):
            if i == j:
                similarity_matrix[i, j] = 1.0
            else:
                # 计算Tanimoto相似性
                similarity = AllChem.DataStructs.TanimotoSimilarity(
                    fingerprints[i], fingerprints[j]
                )
                # 因为矩阵是对称的，所以同时填充 (i, j) 和 (j, i)
                similarity_matrix[i, j] = similarity
                similarity_matrix[j, i] = similarity

    return similarity_matrix


# region p sim
def align_pair(seq1: str, seq2: str, aligner_config: dict) -> float:
    # 在每个并行进程中重新创建一个 Aligner 对象
    try:
        aligner = Align.PairwiseAligner(**aligner_config)
        return aligner.score(seq1.upper(), seq2.upper())
    except Exception as e:
        print(f"Sequences: {seq1}\n{seq2}")  ##测试为什么用新的align算法会失败
        print(f"Error aligning sequences: {e}")
        return 0.0


def get_custom_blosum62_with_U():
    """
    加载标准的BLOSUM62矩阵,并增加对'U'(硒半胱氨酸)的支持。
    'U'的打分规则将完全模仿'C'(半胱氨酸)。
    """
    # 加载标准的BLOSUM62矩阵
    blosum62 = substitution_matrices.load("BLOSUM62")
    # 将其转换为Python字典以便修改
    custom_matrix_dict = dict(blosum62)
    # 获取所有标准氨基酸的字母 (包括 B, Z, X)
    old_alphabet = blosum62.alphabet
    # 为'U'增加打分规则
    for char in old_alphabet:
        # U-X 的得分 = C-X 的得分

        score = custom_matrix_dict.get(("C", char), custom_matrix_dict.get((char, "C")))
        if score is not None:
            custom_matrix_dict[("U", char)] = score
            custom_matrix_dict[(char, "U")] = score
    # U-U 的得分 = C-C 的得分
    custom_matrix_dict[("U", "U")] = custom_matrix_dict[("C", "C")]
    return substitution_matrices.Array(data=custom_matrix_dict)


def calculate_protein_similarity(sequence_list, cpus: int):
    # 定义 Aligner 的配置字典，以便传递给并行任务s
    aligner_config = {
        "mode": "local",
        "substitution_matrix": get_custom_blosum62_with_U(),
        "open_gap_score": -10,
        "extend_gap_score": -0.5,
    }
    num_sequences = len(sequence_list)
    print("--> Pre-calculating self-alignment scores for normalization...")
    self_scores = Parallel(n_jobs=cpus)(
        delayed(align_pair)(seq, seq, aligner_config)
        for seq in tqdm(
            sequence_list, desc="Self-Alignment Pre-computation", leave=False
        )
    )
    # 加上一个很小的数，防止未来出现除以零的错误 (例如空序列导致score为0)
    self_scores = np.array(self_scores, dtype=np.float32) + 1e-8

    # =========================================================================
    # 3. [两两比对阶段] 生成所有唯一的比对任务并并行计算原始分数
    # =========================================================================
    tasks = []
    for i in range(num_sequences):
        for j in range(i, num_sequences):
            # 我们只计算上三角部分，包括对角线
            tasks.append((i, j))

    print("--> Calculating pairwise raw alignment scores...")
    raw_pairwise_scores = Parallel(n_jobs=cpus)(
        delayed(align_pair)(sequence_list[i], sequence_list[j], aligner_config)
        for i, j in tqdm(tasks, desc="Pairwise Alignment", leave=False)
    )

    # =========================================================================
    # 4. [归一化与填充阶段] 使用预计算的分数进行归一化并构建矩阵
    # =========================================================================
    print("--> Populating similarity matrix with normalized scores...")
    similarity_matrix = np.zeros((num_sequences, num_sequences), dtype=np.float32)

    for (i, j), raw_score in zip(
        tqdm(
            tasks,
            desc="Normalizing and Filling Matrix",
            leave=False,
        ),
        raw_pairwise_scores,
    ):
        if i == j:
            similarity_matrix[i, j] = 1.0
        else:
            # 计算归一化的分母
            denominator = np.sqrt(self_scores[i] * self_scores[j])

            # 应用归一化公式
            normalized_score = raw_score / denominator

            # 使用 np.clip 确保分数严格落在 [0, 1] 区间，处理浮点数精度问题
            final_score = np.clip(normalized_score, 0, 1)

            # 因为矩阵是对称的，所以同时填充 (i, j) 和 (j, i)
            similarity_matrix[i, j] = final_score
            similarity_matrix[j, i] = final_score

    return similarity_matrix


# endregion
import numpy as np
import pandas as pd
from sklearn.ensemble import (
    GradientBoostingClassifier,
)  # 使用LightGBM，通常比原生GBDT更快性能更好
from sklearn.metrics import roc_auc_score, average_precision_score


class GBDT_Link_Predictor:
    """
    使用梯度提升模型（LightGBM）在预先计算的节点嵌入上执行链接预测。

    这个类被设计为在 K-Fold 交叉验证循环的单一一折中被调用。
    它封装了特征矩阵构建、模型训练和对单一一折数据的评估。
    """

    def __init__(self, config: dict):
        """
        使用配置初始化预测器。

        Args:
            config (dict): 来自 Hydra 的完整配置对象。
        """
        self.config = config
        # 从 predictor 配置组中获取模型超参数
        self.params = config.predictor
        # 从 runtime 配置组中获取运行时参数（如 seed）
        self.runtime_params = config.runtime

    def predict(
        self, node_embeddings: np.ndarray, train_df: pd.DataFrame, test_df: pd.DataFrame
    ) -> dict:
        """
        为单一一折（Fold）执行训练、预测和评估。

        这个方法是该类的唯一入口点，由 train.py 中的 K-fold 循环调用。

        Args:
            node_embeddings (np.ndarray): 由上游编码器（如 NDLS）生成的节点嵌入矩阵。
                                          其索引应与全局节点ID对应。
            train_df (pd.DataFrame): 包含 ['source', 'target', 'label'] 的训练集。
            test_df (pd.DataFrame): 包含 ['source', 'target', 'label'] 的测试/验证集。

        Returns:
            dict: 一个包含该折评估结果的字典，例如 {'auc': 0.85, 'aupr': 0.75}。
        """
        print("--- [Predictor] Starting GBDT prediction for the current fold ---")

        # 1. 为训练集和测试集分别构建特征矩阵
        print("--> Constructing feature matrices from node embeddings...")
        X_train, y_train = self._create_feature_matrix(node_embeddings, train_df)
        X_test, y_test = self._create_feature_matrix(node_embeddings, test_df)

        print(f"--> Train shape: {X_train.shape}, Test shape: {X_test.shape}")

        # 2. 初始化并训练 LGBM 模型
        # 参数从 conf/predictor/gbdt.yaml 中读取
        print(
            "--> Training GradientBoostingClassifier model on the training set for this fold..."
        )
        model = GradientBoostingClassifier(
            n_estimators=self.params.get("n_estimators", 100),
            max_depth=self.params.get("max_depth", 7),
            subsample=self.params.get("subsample", 1.0),
            learning_rate=self.params.get("learning_rate", 0.1),
            random_state=self.runtime_params.seed,
            verbose=1,
        )
        model.fit(X_train, y_train)

        # 3. 在测试集上进行评估
        print("--> Evaluating model on the test set for this fold...")
        y_pred_proba = model.predict_proba(X_test)[:, 1]

        # 4. 计算评估指标
        auc_score = roc_auc_score(y_test, y_pred_proba)
        aupr_score = average_precision_score(y_test, y_pred_proba)

        # 5. 返回包含该折结果的字典
        # train.py 中的主循环将会收集这些结果
        return {"auc": auc_score, "aupr": aupr_score}

    @staticmethod
    def _create_feature_matrix(
        node_embeddings: np.ndarray, edges_df: pd.DataFrame
    ) -> tuple[np.ndarray, np.ndarray]:
        """
        一个静态辅助方法，用于从节点嵌入和边列表中创建特征矩阵和标签。

        Args:
            node_embeddings (np.ndarray): 完整的节点嵌入矩阵。
            edges_df (pd.DataFrame): 包含 source, target, label 的数据框。

        Returns:
            tuple[np.ndarray, np.ndarray]: (特征矩阵 X, 标签向量 y)
        """
        # 提取 source 和 target 节点的全局ID
        pairs = edges_df[["source", "target"]].values
        labels = edges_df["label"].values

        # 从嵌入矩阵中查找对应节点的嵌入向量
        # 这是一个高效的 NumPy 索引操作
        source_embeds = node_embeddings[pairs[:, 0]]
        target_embeds = node_embeddings[pairs[:, 1]]

        # 将 source 和 target 的嵌入拼接起来，构成边的特征
        # 这是链接预测中的一个标准做法
        features = np.concatenate([source_embeds, target_embeds], axis=1)

        return features, labels
import torch
import torch.nn.functional as F
from torch_geometric.nn import SAGEConv, HeteroConv
from torch_geometric.data import HeteroData


class RGCNLinkPredictor(torch.nn.Module):
    """
    一个端到端的异构图链接预测模型。

    它集成了：
    1. 一个基于 SAGEConv 的异构图编码器 (Encoder)。
    2. 一个用于 DTI 预测的链接解码器 (Decoder)。

    该模型直接与 train.py 中的 HeteroData 对象兼容。
    """

    def __init__(
        self,
        hidden_channels: int,
        out_channels: int,
        num_layers: int,
        dropout: float,
        metadata: tuple,
        target_edge_type: tuple = ("drug", "drug_protein_interaction", "protein"),
    ):
        """
        初始化端到端模型。

        Args:
            hidden_channels (int): GNN隐藏层的维度.
            out_channels (int): 最终节点嵌入的维度 (Decoder也将使用此维度).
            num_layers (int): GNN的层数.
            dropout (float): Dropout 比例.
            metadata (tuple): 异构图的元数据 (node_types, edge_types)，由 train.py 提供.
            target_edge_type (tuple): 需要预测的链接类型.
        """
        super().__init__()

        self.hidden_channels = hidden_channels
        self.out_channels = out_channels
        self.target_edge_type = target_edge_type

        node_types, edge_types = metadata

        # --- 1. 构建异构 GNN 编码器 ---
        self.convs = torch.nn.ModuleList()

        # 第一层：将输入特征映射到隐藏层维度
        # HeteroConv 为每种边类型创建一个 SAGEConv 实例
        conv = HeteroConv(
            {
                edge_type: SAGEConv((-1, -1), hidden_channels)
                for edge_type in edge_types
            },
            aggr="sum",
        )
        self.convs.append(conv)

        # 中间层：隐藏层到隐藏层
        for _ in range(num_layers - 2):
            conv = HeteroConv(
                {
                    edge_type: SAGEConv((-1, -1), hidden_channels)
                    for edge_type in edge_types
                },
                aggr="sum",
            )
            self.convs.append(conv)

        # 最后一层：隐藏层到输出维度 (out_channels)
        conv = HeteroConv(
            {edge_type: SAGEConv((-1, -1), out_channels) for edge_type in edge_types},
            aggr="sum",
        )
        self.convs.append(conv)

        self.dropout = dropout

        # --- 2. 链接解码器 (Decoder) ---
        # 对于 'drug_protein_interaction'，我们使用一个简单的点积解码器。
        # 对于更复杂的 Het-NDLS，这里将是我们的创新点。
        # self.decoder = ... (这里不需要显式的层，直接在 decode 方法中实现)

    def forward(self, x_dict, edge_index_dict):
        """
        执行消息传递以计算节点嵌入。

        Args:
            x_dict (dict): 节点特征字典 {node_type: Tensor}.
            edge_index_dict (dict): 边索引字典 {edge_type: Tensor}.

        Returns:
            dict: 更新后的节点嵌入字典 {node_type: Tensor}.
        """
        if self.training and getattr(self, "_debug_probe_2_done", False) is False:
            print("\n" + "=" * 50)
            print(" " * 15 + "DEBUG PROBE 2: GNN ENCODER INPUT")
            print("=" * 50)
            print("Shapes of node features received by the first GNN layer:")
            for node_type, features in x_dict.items():
                print(f"  - '{node_type}': {features.shape}")
            print("=" * 50 + "\n")
            self._debug_probe_2_done = True  # 设置一个标志，确保只打印一次

        for conv in self.convs[:-1]:
            x_dict = conv(x_dict, edge_index_dict)
            x_dict = {key: F.relu(x) for key, x in x_dict.items()}
            x_dict = {
                key: F.dropout(x, p=self.dropout, training=self.training)
                for key, x in x_dict.items()
            }

        # 最后一层不使用 ReLU 和 Dropout
        x_dict = self.convs[-1](x_dict, edge_index_dict)

        return x_dict

    def decode(self, z_dict, edge_label_index):
        """
        为给定的边 (正样本或负样本) 计算链接存在概率的 logit。

        Args:
            z_dict (dict): 由 forward() 产生的节点嵌入.
            edge_label_index (Tensor): 需要评分的边的索引 [2, num_edges].

        Returns:
            Tensor: 每条边的预测分数 (logits).
        """
        src_type, _, dst_type = self.target_edge_type

        # 提取头节点和尾节点的嵌入
        z_src = z_dict[src_type][edge_label_index[0]]
        z_dst = z_dict[dst_type][edge_label_index[1]]

        # 使用点积作为解码器 (DistMult 的简化版)
        # 这是一个高效且常用的基线解码器
        scores = (z_src * z_dst).sum(dim=-1)
        return scores

    def get_loss(
        self,
        hetero_data: HeteroData,
        edge_label_index: torch.Tensor,
        edge_label: torch.Tensor,
    ):
        """
        端到端的损失计算函数，直接在 train.py 中调用。

        Args:
            hetero_data (HeteroData): 包含图结构和节点特征的训练图.
            edge_label_index (Tensor): 一个 batch 的正/负样本边.
            edge_label (Tensor): 对应边的真实标签 (0 或 1).

        Returns:
            Tensor: 标量损失值.
        """
        # 1. 消息传递 (Encoder)
        # 注意: 这里只使用训练图的边 (hetero_data.edge_index_dict)
        z_dict = self.forward(hetero_data.x_dict, hetero_data.edge_index_dict)

        # 2. 链接预测 (Decoder)
        # 对 batch 中的边进行评分
        scores = self.decode(z_dict, edge_label_index)

        # 3. 计算损失
        loss = F.binary_cross_entropy_with_logits(scores, edge_label.float())

        return loss

    @torch.no_grad()
    def inference(self, hetero_data: HeteroData, edge_label_index: torch.Tensor):
        """
        推理函数，用于评估阶段。返回预测概率。
        """
        # 1. 消息传递
        z_dict = self.forward(hetero_data.x_dict, hetero_data.edge_index_dict)

        # 2. 链接预测
        scores = self.decode(z_dict, edge_label_index)

        # 3. 转换为概率 (0-1)
        probs = torch.sigmoid(scores)

        return probs
import hydra
from omegaconf import DictConfig, OmegaConf
import research_template as rt

# Import all your core logic building blocks
# (We need to refactor data_proc.py and main.py into functions)
from data_processor.main_processor import process_data
from train import train


def is_config_valid(config: DictConfig) -> bool:
    """
    Checks if the experiment configuration is logically valid.

    Rules:
    1. If gtopdb is NOT used (`use_gtopdb: false`), then relations involving 'l'
       (ligand) are FORBIDDEN.
    2. If gtopdb IS used (`use_gtopdb: true`), then relations involving 'l'
       are MANDATORY (at least one 'l' relation must be enabled).

    Args:
        config (DictConfig): The fully composed Hydra configuration object.

    Returns:
        bool: True if the configuration is valid, False otherwise.
    """
    try:
        # --- Rule 1: Check for forbidden relations when gtopdb is off ---
        use_gtopdb = config.data.use_gtopdb

        # Access the dictionary of relation switches
        # The actual config group is 'relations', which contains a 'params' sub-key,
        # which in turn contains 'include_relations'. Let's use a robust access method.
        # UPDATE: Based on your provided config structure, the path is simpler.
        include_relations = config.relations.flags

        # Define which relation keys are considered 'ligand-related'
        ligand_related_keys = ["lp_interaction", "ll_similarity", "dl_similarity"]

        # Check if any ligand-related relation is enabled
        any_l_relation_enabled = any(
            include_relations.get(key, False) for key in ligand_related_keys
        )

        if not use_gtopdb:
            if any_l_relation_enabled:
                print(
                    "[CONFIG INVALID] Run skipped: Ligand relations (e.g., lp, ll, dl) are enabled, but 'use_gtopdb' is false."
                )
                return False

        # --- Rule 2: Check for mandatory relations when gtopdb is on ---
        else:  # This means use_gtopdb is True
            if not any_l_relation_enabled:
                print(
                    "[CONFIG INVALID] Run skipped: 'use_gtopdb' is true, but no ligand-related relations are enabled in the config."
                )
                return False

    except Exception as e:
        # This will catch errors if the config structure is unexpected (e.g., 'include_relations' is missing)
        print(
            f"[CONFIG CHECK FAILED] Could not validate config due to an error: {e}. Skipping run."
        )
        return False

    # If all checks pass, the configuration is valid
    return True


@hydra.main(config_path="../conf", config_name="config", version_base=None)
def run_experiment(cfg: DictConfig):
    """
    Main entry point for the experiment, driven by Hydra.
    'cfg' is the fully composed configuration object.
    """
    # --- 1. Print and Setup ---
    print("--- Fully Composed Configuration ---")
    print(OmegaConf.to_yaml(cfg))
    if not is_config_valid(cfg):
        return
    config_dict = cfg

    rt.set_seeds(config_dict.runtime.seed)
    rt.setup_dataset_directories(config_dict)  # Handles directory creation and cleaning

    process_data(config_dict)
    train(config_dict)


if __name__ == "__main__":
    run_experiment()
import torch
import sys
# [关键] 手动将项目根目录添加到Python路径中
sys.path.append('.') 
from src.data_utils.loaders import load_graph_data_for_fold, load_labels_for_fold
from torch_geometric.loader import LinkNeighborLoader

def main_debug():
    print("--- Starting Minimal Loader Debug ---")
    
    # 模拟一个简化的config对象
    class SimpleConfig:
        def __init__(self):
            self.data = {'primary_dataset': 'DrugBank', 'use_gtopdb': False, ...} # 补全
            self.training = {'evaluation': {'...': ...}} # 补全
            self.predictor = {'params': {'num_layers': 2}}

    config_mock = SimpleConfig()
    
    # 1. 加载数据
    hetero_graph = load_graph_data_for_fold(config_mock, 1)
    train_df, _ = load_labels_for_fold(config_mock, 1)
    
    # 2. 准备监督边
    target_edge_type = ("drug", "drug_protein_interaction", "protein")
    train_pos_df = train_df[train_df['label'] == 1]
    train_edge_label_index = torch.from_numpy(train_pos_df[['source', 'target']].values).t().contiguous().long()

    # 3. 实例化Loader
    try:
        print("--> Instantiating Loader...")
        loader = LinkNeighborLoader(
            data=hetero_graph,
            num_neighbors=[-1] * 2,
            edge_label_index=(target_edge_type, train_edge_label_index),
            batch_size=512,
            shuffle=True,
            neg_sampling_ratio=1.0,
            num_workers=0
        )
        print("--> Loader instantiated SUCCESSFULLY.")
    except Exception as e:
        print(f"!!! FAILED during Loader instantiation: {e}")
        return

    # 4. 尝试取出第一个batch
    try:
        print("--> Attempting to fetch the first batch...")
        first_batch = next(iter(loader))
        print("--> First batch fetched SUCCESSFULLY!")
        print("Batch content:", first_batch)
    except Exception as e:
        print(f"!!! FAILED when fetching the first batch: {e}")

if __name__ == "__main__":
    main_debug()import numpy as np
import pandas as pd
import torch.nn.functional as F
from torch_geometric.data import HeteroData
import torch
from encoders.ndls_homo_encoder import NDLS_Homo_Encoder
from predictors.gbdt_predictor import GBDT_Link_Predictor
import research_template as rt
from omegaconf import DictConfig
import torch.utils.data
import mlflow
import traceback
import hydra  # noqa: F401
from tqdm import tqdm
from predictors.rgcn_link_predictor import RGCNLinkPredictor
from sklearn.metrics import roc_auc_score, average_precision_score
from torch_geometric.loader import LinkNeighborLoader
from encoders.ndls_homo_utils import convert_hetero_to_homo
from data_utils.loaders import (
    load_graph_data_for_fold,
    load_labels_for_fold,
    convert_df_to_local_tensors,  # noqa: F401
    create_global_to_local_maps,
)

target_edge_type = ("drug", "drug_protein_interaction", "protein")


def run_workflow_for_fold(config, hetero_graph, train_df, test_df, device, maps):
    """根据配置分派并执行相应的工作流"""
    paradigm = config.training.paradigm

    if paradigm == "two_stage":
        return run_two_stage_workflow(config, hetero_graph, train_df, test_df, device)
    elif paradigm == "end_to_end":
        return run_end_to_end_workflow(
            config, hetero_graph, train_df, test_df, device, maps
        )
    else:
        raise ValueError(f"Unknown paradigm: {paradigm}")


# region two stage
def run_two_stage_workflow(config, hetero_graph, train_df, test_df, device):
    print("\n--- Running Two-Stage (Encoder + Predictor) Workflow ---")
    # 5a. 运行编码器
    encoder_name = config.encoder.name
    if encoder_name == "ndls_homo":
        adj, features, _ = convert_hetero_to_homo(hetero_graph)
        encoder = NDLS_Homo_Encoder(config, device)
        encoder.fit(adj, features)
        node_embeddings = encoder.get_embeddings()
    else:
        raise NotImplementedError(f"Encoder '{encoder_name}' not supported.")

    # 5b. 运行预测器
    predictor_name = config.predictor.name
    if predictor_name == "gbdt":
        predictor = GBDT_Link_Predictor(config)
        print(predictor.params)
        # [核心变化] 将加载好的数据框直接传进去
        return predictor.predict(node_embeddings, train_df, test_df)
    else:
        raise NotImplementedError(f"Predictor '{predictor_name}' not supported.")


# end region


# region e2e
def run_end_to_end_workflow(
    config: DictConfig,
    hetero_graph: HeteroData,
    train_df: pd.DataFrame,
    test_df: pd.DataFrame,
    device: torch.device,
    maps: dict,
):
    print("\n--- Running End-to-End (Mini-Batch Inductive) Workflow ---")
    target_edge_type = ("drug", "drug_protein_interaction", "protein")
    # --- 1. [数据净化] 确保所有图数据都在CPU上，且类型正确 ---
    print("--> Purifying and validating HeteroData object before Loader...")

    # a. 将图的所有张量都显式地移动到CPU
    for store in hetero_graph.stores:
        for key, value in store.items():
            if torch.is_tensor(value):
                store[key] = value.to("cpu")

    # b. 显式地将所有edge_index转换为torch.long类型
    for edge_type in hetero_graph.edge_types:
        hetero_graph[edge_type].edge_index = hetero_graph[edge_type].edge_index.long()

    print("--> HeteroData object successfully purified on CPU.")
    # --- [核心修复] 在调用Loader之前，将所有监督边，从全局ID转换为局部ID ---
    print(
        "--> Preparing supervision edges with LOCAL indices for LinkNeighborLoader..."
    )
    src_type, _, dst_type = target_edge_type

    # a. 训练监督边 (只使用正样本)
    train_pos_df = train_df[train_df["label"] == 1]
    train_src_local = torch.tensor(
        [maps[src_type][gid] for gid in train_pos_df["source"]], dtype=torch.long
    )
    train_dst_local = torch.tensor(
        [maps[dst_type][gid] for gid in train_pos_df["target"]], dtype=torch.long
    )
    train_edge_label_index_local = torch.stack([train_src_local, train_dst_local])

    # b. 测试监督边
    test_src_local = torch.tensor(
        [maps[src_type][gid] for gid in test_df["source"]], dtype=torch.long
    )
    test_dst_local = torch.tensor(
        [maps[dst_type][gid] for gid in test_df["target"]], dtype=torch.long
    )
    test_edge_label_index_local = torch.stack([test_src_local, test_dst_local])
    test_labels = torch.from_numpy(test_df["label"].values).float()

    # --- [最终的断言] 验证我们转换后的局部ID，没有越界 ---
    assert train_src_local.max() < hetero_graph[src_type].num_nodes
    assert train_dst_local.max() < hetero_graph[dst_type].num_nodes
    assert test_src_local.max() < hetero_graph[src_type].num_nodes
    assert test_dst_local.max() < hetero_graph[dst_type].num_nodes
    print("✅ Local ID validation successful.")

    # --- 实例化数据加载器 (现在所有ID都是自洽的局部ID) ---
    train_loader = LinkNeighborLoader(
        data=hetero_graph,
        num_neighbors=[-1] * config.predictor.params.num_layers,
        edge_label_index=(
            target_edge_type,
            train_edge_label_index_local,
        ),  # <-- 使用局部ID
        batch_size=config.training.get("batch_size", 512),
        shuffle=True,
        neg_sampling_ratio=1.0,
        num_workers=0,
    )
    # --- 2. 模型实例化 ---
    # 我们继续使用手动实例化，因为它最清晰、最可控
    print("--> Instantiating model...")
    model = RGCNLinkPredictor(
        hidden_channels=config.predictor.params.hidden_channels,
        out_channels=config.predictor.params.out_channels,
        num_layers=config.predictor.params.num_layers,
        dropout=config.predictor.params.dropout,
        metadata=hetero_graph.metadata(),
    ).to(device)
    print("--> Model instantiation successful!")
    optimizer = torch.optim.Adam(model.parameters(), lr=config.training.learning_rate)

    # --- 3. [数据净化] 准备监督边，并确保类型正确 ---
    print("--> Preparing supervision edges with correct data types...")
    # 我们硬编码的target_edge_type，与模型内部一致
    target_edge_type = ("drug", "drug_protein_interaction", "protein")

    # a. 训练监督边 (只使用正样本，Loader会自动处理负采样)
    train_pos_df = train_df[train_df["label"] == 1]
    train_edge_label_index = (
        torch.from_numpy(train_pos_df[["source", "target"]].values)
        .t()
        .contiguous()
        .long()
    )  # <-- [关键] 强制为.long()

    # b. 测试监督边
    test_edge_label_index = (
        torch.from_numpy(test_df[["source", "target"]].values).t().contiguous().long()
    )  # <-- [关键] 强制为.long()
    test_labels = torch.from_numpy(
        test_df["label"].values
    ).float()  # <-- [关键] 强制为.float()

    # --- 4. [最终的断言] 在调用Loader之前，做最后一次检查 ---
    assert hetero_graph.is_undirected(), (
        "FATAL: Graph is not undirected before passing to Loader!"
    )

    # --- 5. 实例化数据加载器 ---
    print("--> Initializing LinkNeighborLoader in single-process mode...")
    train_loader = LinkNeighborLoader(
        data=hetero_graph,
        num_neighbors=[-1] * config.predictor.params.num_layers,
        edge_label_index=(target_edge_type, train_edge_label_index),
        batch_size=config.training.get("batch_size", 512),
        shuffle=True,
        neg_sampling_ratio=1.0,
        num_workers=0,  # <-- [关键] 使用单进程进行调试
    )

    # --- 6. 训练循环 ---
    print("--> Starting mini-batch model training...")
    for epoch in tqdm(range(config.training.epochs), desc="Epochs"):
        total_loss = 0
        total_examples = 0
        model.train()
        for batch in tqdm(train_loader, desc="Batches", leave=False):
            batch = batch.to(device)
            optimizer.zero_grad()

            z_dict = model.forward(batch.x_dict, batch.edge_index_dict)
            scores = model.decode(z_dict, batch[target_edge_type].edge_label_index)
            labels = batch[target_edge_type].edge_label.float()

            loss = F.binary_cross_entropy_with_logits(scores, labels)
            loss.backward()
            optimizer.step()

            total_loss += loss.item() * scores.size(0)
            total_examples += scores.size(0)

        # [优化] 避免除以零的错误
        if total_examples > 0:
            avg_loss = total_loss / total_examples
            if epoch % 10 == 0:
                print(f"Epoch {epoch}: Avg Loss = {avg_loss:.4f}")

    # --- 7. 评估 ---
    print("--> Starting model evaluation...")
    test_loader = LinkNeighborLoader(
        data=hetero_graph,
        num_neighbors=[-1] * config.predictor.params.num_layers,
        edge_label_index=(
            target_edge_type,
            test_edge_label_index_local,
        ),  # <-- 使用局部ID
        edge_label=test_labels,
        batch_size=config.training.get("batch_size", 512),
        shuffle=False,
        neg_sampling_ratio=0.0,
        num_workers=0,
    )

    all_preds = []
    all_labels = []
    model.eval()
    with torch.no_grad():
        for batch in tqdm(test_loader, desc="Evaluating"):
            batch = batch.to(device)
            z_dict = model.forward(batch.x_dict, batch.edge_index_dict)
            scores = model.decode(z_dict, batch[target_edge_type].edge_label_index)

            all_preds.append(torch.sigmoid(scores).cpu())
            all_labels.append(batch[target_edge_type].edge_label.cpu())

    preds = torch.cat(all_preds, dim=0).numpy()
    labels = torch.cat(all_labels, dim=0).numpy()

    # [优化] 增加一个检查，防止在没有正或负样本时AUC计算报错
    if len(np.unique(labels)) < 2:
        print(
            "    -> WARNING: Test set contains only one class. AUC/AUPR cannot be computed."
        )
        auc, aupr = 0.0, 0.0
    else:
        auc = roc_auc_score(labels, preds)
        aupr = average_precision_score(labels, preds)

    return {"auc": auc, "aupr": aupr}


# end region

# region main


def train(config: DictConfig):
    tracker = rt.MLflowTracker(config)
    # --- Start of protected block for MLflow ---
    try:
        # ===================================================================
        # 2. Setup: Start MLflow Run, Set Environment, and Get Config
        # ===================================================================
        tracker.start_run()  # This also logs all relevant parameters
        device = torch.device(
            config["runtime"]["gpu"] if torch.cuda.is_available() else "cpu"
        )

        # Get the primary switches for the experiment from config
        training_config = config["training"]
        encoder_name = config.encoder.name
        # Use .get() for the optional predictor
        predictor_name = config.predictor.name

        paradigm = training_config.paradigm
        data_variant = "gtopdb" if config.data.use_gtopdb else "baseline"

        # 3. Startup Log
        print("\n" + "=" * 80)
        print(" " * 20 + "Starting DTI Prediction Experiment")
        print("=" * 80)
        print("Configuration loaded for this run:")
        print(f"  - Paradigm (Inferred): '{paradigm}'")
        print(f"  - Primary Dataset:     '{config['data']['primary_dataset']}'")
        print(f"  - Data Variant:        '{data_variant}'")
        print(f"  - Encoder:             '{encoder_name}'")
        print(f"  - Predictor:           '{predictor_name or 'N/A'}'")
        print(f"  - Use Relations:   '{config.relations.name}'")
        print(f"  - Seed:                {config['runtime']['seed']}")
        print(f"  - Device:              {device}")
        print("=" * 80 + "\n")

        k_folds = config.training.evaluation.k_folds
        all_fold_aucs = []
        all_fold_auprs = []

        for fold_idx in range(1, k_folds + 1):
            print("\n" + "#" * 80)
            print(f"#{' ' * 28}PROCESSING FOLD {fold_idx} / {k_folds}{' ' * 28}#")
            print("#" * 80 + "\n")
            global_to_local_maps = create_global_to_local_maps(config)
            # 1. Load data specific to the current fold
            hetero_data = load_graph_data_for_fold(
                config, fold_idx, global_to_local_maps
            )
            train_df, test_df = load_labels_for_fold(config, fold_idx)

            # 2. --- Workflow Dispatcher (based on paradigm) ---
            fold_results = run_workflow_for_fold(
                config, hetero_data, train_df, test_df, device, global_to_local_maps
            )

            # 3. Store results for the current fold
            if fold_results:
                # Assuming the predictor returns a dictionary like {'auc': 0.9, 'aupr': 0.8}
                print(
                    f"--> Fold {fold_idx} Results: AUC = {fold_results['auc']:.4f}, AUPR = {fold_results['aupr']:.4f}"
                )
                all_fold_aucs.append(fold_results["auc"])
                all_fold_auprs.append(fold_results["aupr"])

        # 4. After the loop, log the aggregated cross-validation results
        ### [MODIFIED] ###
        # Logging now happens once at the very end with the CV results.
        if all_fold_aucs and all_fold_auprs:
            print("\n" + "=" * 80)
            print(" " * 25 + "Cross-Validation Summary")
            print("=" * 80)
            print(
                f"  - Mean AUC:  {np.mean(all_fold_aucs):.4f} +/- {np.std(all_fold_aucs):.4f}"
            )
            print(
                f"  - Mean AUPR: {np.mean(all_fold_auprs):.4f} +/- {np.std(all_fold_auprs):.4f}"
            )
            tracker.log_cv_results(all_fold_aucs, all_fold_auprs)

    except Exception as e:
        # ... (error handling remains the same) ...
        print(f"\n!!! FATAL ERROR: Experiment run failed: {e}")
        if tracker.is_active:
            mlflow.set_tag("run_status", "FAILED")
            mlflow.log_text(traceback.format_exc(), "error_traceback.txt")
        raise
    finally:
        # ===================================================================
        # 7. Teardown: Always end the MLflow run
        # ===================================================================
        tracker.end_run()
        print("\n" + "=" * 80)
        print(" " * 27 + "Experiment Run Finished")
        print("=" * 80 + "\n")


# end region
