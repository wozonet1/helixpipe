### **ADR-005: 演进为中心化的实体校验与丰富化流水线**

**状态：** 已提议 (Proposed)

**日期：** 2025-11-04

#### **背景 (Context)**

在完成 **ADR-003（规范化交互模型）** 的设计后，我们拥有了一个通用的数据模型，能够表示各种异构交互。然而，在实现过程中，我们面临一个新的架构决策点：<strong>“实体校验（Validation）”**和**“实体丰富化（Enrichment）</strong>”的逻辑应该放在哪里？

- **实体校验**包括：ID 白名单验证、结构语法（SMILES/序列）有效性检查、理化性质过滤等。
- **实体丰富化**主要是指：对于只有 ID 的实体（例如来自 BRENDA），需要调用 StructureProvider 来获取其结构信息。

这个决策的核心在于，如何在\*\*“尽早失败（Fail Fast）”**的性能原则和**“架构一致性与单一职责”\*\*的设计原则之间做出权衡。

#### **决策 (Decision)**

我们将采用一个**严格的“流水线中心化（Pipeline-centric）”模型**。所有通用的实体校验和丰富化逻辑将被整合到一个统一的、由主流水线 (main_pipeline) 负责编排的中心化服务中。

**具体架构变更如下：**

- **Processor 职责纯化：**

  - 所有 \*Processor 的职责被严格限定为\*\*“翻译”**和执行**“领域专属过滤”\*\*（如亲和力阈值）。
  - 它们**不再负责**任何通用的校验（如 ID 白名单、结构语法、理化性质）或在线的结构丰富化（调用 StructureProvider）。
  - Processor 的产出是一个“半成品”DataFrame，它包含了尽可能多的原始信息（包括可能无效的实体和结构），以供下游统一处理。

- **main_pipeline 成为总编排器：**

  - main_pipeline 将在聚合所有 Processor 的输出后，执行一个统一的、顺序明确的\*\*“实体处理”\*\*流程：
    a. **提取（Extract）：** 从所有交互中构建一个全局唯一的“实体清单”。
    b. **丰富化（Enrich）：** 检查清单中哪些实体缺少结构信息，**统一、去重后**调用 StructureProvider 进行一次性的数据补全。
    c. **校验（Validate）：** 将完整的实体清单（包含所有 ID 和结构）**一次性**地送入一个新的、中心化的 EntityValidator 服务。
    d. **过滤（Filter）：** 使用 EntityValidator 返回的“纯净实体集”，对原始的交互列表进行最终过滤。

- **EntityValidator 服务诞生：**

  - 创建一个新的、高内聚的服务 EntityValidator。
  - 它将整合原先分散在 purifier, filter, id_validation_service 中的所有通用校验逻辑，成为项目中关于“实体质量标准”的**唯一事实来源（Single Source of Truth）** 。

**架构设计原则 (Architectural Principles):**

这次向“流水线中心化”模型的演进，其本质是将我们的数据处理系统从一个“专用的 DTI 工具”，升级为一个**真正的“通用异构数据处理引擎”** 。该决策遵循了以下核心设计原则：

- **统一的抽象：** 废弃为特定交互（DTI）设计的、紧耦合的数据模型，转而采用“规范化交互格式”这一**元模型（Meta-model）** 。该模型只描述“交互”本身 (source, target, relation)，使其能够无差别地容纳 DTI, PPI, 乃至未来任何类型的异构关系。
- **关注点分离 (SoC)：** 严格划分了流水线中不同模块的职责：

  - **Processor:** 只负责**领域翻译**（将原始数据转换为规范化格式）和**领域筛选**（应用与数据源强相关的业务规则）。
  - **main_pipeline:** 只负责**流程编排**（Collect -\> Enrich -\> Validate -\> Filter -\> Dispatch）。
  - **Services (如 EntityValidator):** 只负责执行**通用任务**（对所有实体施加统一的质量标准）。

- **单一事实来源 (SSOT)：**

  - **StructureProvider** 成为所有“实体结构”的唯一权威来源，解决了“双重真理”问题。
  - **EntityValidator** 成为所有“实体通用质量标准”的唯一权威来源，保证了全局数据的一致性。

---

#### **备选方案 (Considered Options)**

**1. “处理器中心化（Processor-centric）”模型**

- **方案描述：** 将所有校验和丰富化逻辑**下沉**到每个 Processor 内部。每个 Processor 都成为一个能独立产出“纯净、有结构”数据的全能模块。
- **优点：**

  - **极致的“Fail Fast”：** 无效数据在最早的阶段就被清除，理论上性能最高。
  - **高度封装：** 每个 Processor 都像一个独立的微服务。

- **缺点：**

  - **重复的在线请求（致命缺陷）：** 如果多个数据源包含同一个蛋白质 ID，会导致对 StructureProvider 的多次重复调用和缓存查询，在强制刷新缓存时会产生大量冗余网络请求。
  - **校验逻辑分散：** “什么是合格实体”的定义被分散到多个 Processor 的实现中，违反了单一事实来源原则，未来修改规则时极易出错。

- **决策理由：** 该方案为了追求局部的性能优化，牺牲了系统的整体数据一致性和架构的可维护性，其“重复请求”的缺陷在实际应用中是不可接受的，因此予以否-决。

**2. 混合模型**

- **方案描述：** 在 Processor 内部进行部分“离线”校验（如理化性质），在 main_pipeline 中进行统一的“在线”校验（如白名单和结构获取）。
- **优点：** 试图在“Fail Fast”和“一致性”之间取得平衡。
- **缺点：** **引入了巨大的复杂性。** main_pipeline 需要维护一个复杂的校验状态（“哪些实体已经被部分校验过了？”），这会使代码逻辑变得混乱、脆弱且难以理解。
- **决策理由：** 架构设计应优先追求简单和清晰。混合模型带来的复杂性成本远高于其带来的性能收益，予以否决。

#### **后果 (Consequences)**

**正面影响：**

- **架构的极致清晰：** 形成了 Processor（翻译） -\> main_pipeline（编排） -\> Services（执行）的黄金三角模式，职责划分达到了前所未有的清晰度。
- **保证了数据一致性与质量：** 所有实体都经过同一个“中央质检”流程，从根本上保证了进入下游模型的数据质量是统一和可靠的。
- **资源利用效率最大化：** 通过在 main_pipeline 中进行统一的、去重后的在线数据请求，彻底避免了对同一实体的重复查询，节省了网络资源和时间。
- **可维护性与可扩展性：** 修改校验规则或增加新的校验步骤，只需要在一个地方（EntityValidator）进行，极大地降低了维护成本。

- **实现了真正的“异构化”与可扩展性：**

  - 我们的系统现在遵循**开闭原则**。引入新的异构数据源（如基因-疾病），只需要**新增**一个对应的 Processor 作为“插件”，而**无需修改**任何核心的编排和校验逻辑。这从根本上提升了系统的可扩展性。

- **增强了灵活性与稳定性：**

  - 系统现在对 Processor 输出格式的变化具有很高的**容忍度**。例如，为交互记录增加一个新的属性（如 confidence_score），只需要修改 Processor 和最终消费该属性的下游模块（如 GraphBuilder），中间的核心清洗流水线完全不受影响，不会引发连锁修改。

- **清晰的上下游解耦：**

  - **“纯数据清洗”** （Stage 1-5）和\*\*“下游任务构建”\*\*（Stage 6 及以后）被完美分离。这使得数据工程团队和模型工程团队可以独立迭代，互不阻塞。数据清洗的逻辑是稳定的，而下游的模型构建、训练等任务可以灵活多变。

**负面影响/权衡：**

- **牺牲了局部的“Fail Fast”：** “不纯粹”的数据会在流水线中多流转一个阶段。我们接受这个微小的、可控的性能代价，以换取整个架构在一致性、健壮性和可维护性上的巨大收益。
- **main_pipeline 职责加重：** main_pipeline 不再是一个简单的脚本，而是成为了一个核心的、负责复杂编排的“大脑”。这是合理的，因为“编排”正是其核心职责。

**最终结论：**
这次演进，使我们的数据流水线从一个“分布式决策”的模型，升级为了一个“中央集权式决策”的模型。这在处理需要全局信息一致性的复杂数据整合任务时，是一个更高级、更健壮的架构模式。
